# Pipeline System Documentation

This document provides an overview of the pipeline system for data ingestion, transformation, and processing.

## Overview

The pipeline system handles automated data processing workflows including:

- Data scraping from external sources
- Data transformation and normalization
- Image processing and optimization
- Data ingestion into staging and production tables

## Architecture

The pipeline follows a three-layer architecture:

- **Routes** (`src/routes/v1/pipeline.ts`) - HTTP endpoints for pipeline management
- **Services** (`src/services/`) - Business logic for pipeline operations
- **DAO** (`src/dao/pipeline/`) - Data access layer for pipeline entities

## Queue Handling

The pipeline uses different queue mechanisms depending on the environment:

### Production (AWS)

- **SQS + Database**: Jobs are queued to both AWS SQS and the database
- **Automatic Processing**: SQS triggers automatic job processing
- **Scalability**: Multiple workers can process jobs concurrently
- **Reliability**: SQS provides message durability and retry logic

### Localhost/Development

- **Database Only**: Jobs are queued directly to the `pipeline.queue` table
- **Manual Processing**: Jobs must be triggered manually via API endpoint
- **Single Worker**: Processes jobs sequentially from the database
- **No SQS Dependencies**: Works without AWS infrastructure

### Queue Configuration

**Environment Variables:**

- `PIPELINE_ENABLE_QUEUES=true` - Enables SQS (production only)
- `PIPELINE_JOBS_QUEUE_URL` - SQS queue URL for job processing
- `PIPELINE_BATCH_SIZE` - Number of jobs to process per batch
- `PIPELINE_MAX_RETRIES` - Maximum retry attempts for failed jobs

**Manual Processing:**

```bash
POST /api/v1/pipeline/queue/process
```

### Queue States

- `queued` - Job waiting to be processed
- `running` - Job currently being processed
- `completed` - Job finished successfully
- `failed` - Job failed with error

## Job Types

The pipeline supports several job types:

### Scrape Jobs

- Extract data from external sources
- Configurable scraping frequency per DMO
- Handles various data formats (JSON, XML, CSV)

### Transform Jobs

- Normalize data from different sources
- Apply field mappings and validation rules
- Generate record hashes for duplicate detection

### Ingest Jobs

- Load transformed data into staging tables
- Validate data integrity
- Prepare for production promotion

### Image Processing Jobs

- Download and optimize images
- Generate thumbnails and variants
- Upload to S3 storage

## Database Schema

### Pipeline Schema (`pipeline.*`)

- `queue` - Job queue table
- `jobs` - Job execution history
- `files` - Processed data files
- `configs` - Source configurations
- `errors` - Error logging

### Staging Schema (`staging.*`)

- `listings` - Transformed listing data
- `listing_info` - Additional listing details
- `locations` - Geographic data
- `contact_info` - Contact information
- `listing_images` - Image metadata
- `attributes` - Listing attributes
- `opening_hours` - Business hours
- `opening_dates` - Special dates

## API Endpoints

### Queue Management

- `GET /api/v1/pipeline/queue` - List queued jobs
- `POST /api/v1/pipeline/queue` - Add job to queue
- `POST /api/v1/pipeline/queue/process` - Process queue manually

### Job Management

- `GET /api/v1/pipeline/jobs` - List job history
- `GET /api/v1/pipeline/jobs/:id` - Get job details

### File Management

- `GET /api/v1/pipeline/files` - List processed files
- `GET /api/v1/pipeline/files/:id` - Get file details

### Staging Data

- `GET /api/v1/pipeline/staging/listings` - List staging listings
- `POST /api/v1/pipeline/staging/promote` - Promote to production

## Configuration

### Source Configuration

Each data source requires configuration in `pipeline.configs`:

- `source_type` - Type of data source (API, web scraping, file upload)
- `base_url` - Base URL for API endpoints
- `scrape_config` - Scraping configuration (selectors, pagination)
- `field_mappings` - Field mapping rules
- `scrape_frequency` - How often to scrape (daily, weekly, etc.)

### DMO Configuration

DMOs can be configured for automated scraping:

- `source_type` - Type of data source
- `base_url` - Base URL for scraping
- `scrape_config` - Scraping parameters
- `scrape_frequency` - Update frequency
- `last_scrape_at` - Last successful scrape timestamp

## Error Handling

### Queue Errors

- Failed jobs are marked with error messages
- Retry logic with exponential backoff
- Dead letter queue for permanently failed jobs

### Processing Errors

- Detailed error logging in `pipeline.errors`
- Rollback capabilities for failed transformations
- Notification system for critical failures

## Monitoring

### Dashboard Metrics

- Queue depth and processing rate
- Job success/failure rates
- Processing time statistics
- Error frequency and types

### Logging

- Structured logging with correlation IDs
- Performance metrics for each job type
- Audit trail for data changes

## Security

### Access Control

- RBAC permissions for pipeline operations
- Scoped permissions for DMO-specific access
- API key authentication for external sources

### Data Protection

- Encryption in transit and at rest
- Secure credential storage
- Data anonymization for sensitive information

## Development

### Local Setup

1. Ensure PostgreSQL is running
2. Run database migrations
3. Set up environment variables
4. Start the API server
5. Use manual queue processing for testing

### Testing

- Unit tests for service layer
- Integration tests for queue processing
- End-to-end tests for complete workflows

## Troubleshooting

### Common Issues

- **Queue not processing**: Check `PIPELINE_ENABLE_QUEUES` setting
- **SQS connection errors**: Verify AWS credentials and region
- **Database connection issues**: Check PostgreSQL configuration
- **Job failures**: Review error logs in `pipeline.errors`

### Debugging

- Enable debug logging with `LOG_LEVEL=debug`
- Check queue status via API endpoints
- Monitor database for stuck jobs
- Review SQS metrics in AWS Console

## Future Enhancements

- **Real-time Processing**: WebSocket notifications for job status
- **Advanced Scheduling**: Cron-based job scheduling
- **Data Quality Metrics**: Automated data quality scoring
- **Machine Learning**: Intelligent duplicate detection
- **Multi-tenant Support**: Isolated processing per DMO
