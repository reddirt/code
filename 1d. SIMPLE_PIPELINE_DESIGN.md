# Listings Pipeline Design
**goal:**  Enterprise grade listings pipeline with a graphical interface for managing data pipeline.  Required to scale our content very quickly, and keep content up to date with client data updates and repeated scraping.

## Pipeline FLow
```
Configure Step
    • create import/mapping/transform options and schedule for this dmo+import type
    ↓
Import Step
    • Upload csv or run api import script or run website scrape script etc.
    • determine source_id (also used as dedup key)
    • add metadata columns
    • save raw file as .csv or .json
    ↓
Transform Step
    • Lookup bridge/public
    • Geolocation if needed
    • Validation / normalization
    • Compute hashes
    • Save to stage tables (optionally save JSON)
    ↓
Manual Review Step
    • Approve/reject rows, updates, override manual updates
    • Set ok_to_promote flags
    ↓
Promotion Step
    • Insert/update/delete into public tables
```
---


## Pipeline Details

1. Configure Step
- Purpose: Set up import/mapping/transform rules and schedule for a given DMO + import type.
- Actions:
- Define source columns → DB field mappings.
- Define validation rules (required fields, data types, business constraints).
- Optional: set dedupe / conflict resolution policy.

2. Import Raw File
- Purpose: Capture raw source data.
- Actions:
- Upload CSV / JSON, run API import, or scrape website.
- Determine source_id.
- Add metadata columns: rec_num, source_id, dmo_id, timestamp, etc.
- Save raw file for audit / retry purposes.
- **Note:** Raw file is immutable; all transformations happen later.

3. Transform Step
- Purpose: take raw data and get ready for promote step
- Actions:
- Map raw fields to DB fields
- Normalize, validate, geolocate
- Compute hashes
- Do match lookups, content update lookups, deletion lookups
- Prepare action flags for promotion

3a. Field Mapping & Normalization
- Map raw columns → staging table columns (listings, listing_info, dependent objects).
- Normalize fields:
- Trim whitespace
- Lowercase/uppercase consistently
- Remove special characters
- Normalize after mapping, before hashing.

3b. Lookup & Geolocation
- Lookup each row:
	1.	Bridge table (source_id + dmo_id) → get listing_internal_id.
	2.	If not found → match_hash lookup in public.listings.
	3.	If not found → geolocate (lat/lng), listing_internal_id left null.
	4.	Save geolocation & normalized address in stage table.

Partial transforms are supported: if a job fails mid-way, already-looked-up geolocations and hashes are persisted in stage table, avoiding repeat API calls. If retry transform, read raw import, skip over already transformed records and start transforming at last failed record.

3c. Hash Fields & Dedupe Key
- Compute content hashes
- source_id is used for dedup
- Hashes used for:
- Content comparison for updates

3d. Listing Info & Dependent Objects
- Lookup listing_info by listing_internal_id + dmo_id:
- Not found → listing_info_action = 'insert', dependent objects 'insert'
- Found → compare listing_info_content_hash → 'update' / 'skip'
- For dependent objects (contact_info, images, hours):
- If listing_info 'insert' → all 'insert'
- Else → compare hashes → 'update' / 'skip'

3e. Duplicate Detection
- Use source_id as dedup key
- Flag duplicates:
- One “primary” row → OK to promote
- Others → 'skip' or require manual review
- Prevent duplicates from being promoted to production.

3f. Detect Deletions / Soft-Deletes
- For each listing_info under the dmo:
- If no matching staging row, then 
    - lookup parent listing
    - add record with listing and listing_info data
    - listing_info action = 'delete'
    - if the listing has no other listing_info records, then set listing action = delete
- Mark both for manual review

3g. Optional Validation / Business Rules
- Data type checks: lat/lng bounds, emails, phones
- Required fields: per DMO rules
- Business logic: category restrictions, geographic bounds
- Quality scoring (optional)

3h. Stage Table Columns
- Raw + normalized fields
- Lat/lng (original + rounded)
- Hash fields
- Listing & dependent object actions (insert, update, skip, delete)
- Flags for manual review: ok_to_promote, manually_modified
- Audit fields: imported_from, created_at, updated_at

4. Manual Review
- Display stage rows:
- New listings
- Updates
- Duplicates
- Deletions
- Allow reviewers to:
- Approve / reject changes
- Override actions
- Change ok_to_promote flags

5. Promotion Step
- Iterate rows with ok_to_promote = TRUE:
- Listing:
- 'insert' → insert
- 'update' → update
- 'delete' → set public.listings.listing_visible = false, listing_status = 'missing_from_import'
- Listing Info:
- 'insert' / 'update'
- 'delete' → set public.listing_info.info_visible = false,  info_status = 'missing_from_import'
- Dependent Objects:
- 'insert' / 'update'
- Reset manually_modified flag if overridden

6. Key Advantages
- Stage table is single source of truth for review + promotion.
- No need for separate transform/validation files (optionally save JSON for audit).
- Partial transforms possible; retries do not repeat geolocation/lookups unnecessarily.
- Promotion decisions are deterministic; no runtime logic needed.
- Supports duplicates, deletions, soft-deletes, manual overrides.



## Transform Flow with Duplicates and Deletions

1. Lookup & Match
- Lookup listing in bridge table by source_id + dmo_id.
- If not found in bridge → use match_hash to lookup in public.listings.
- If no match is found → set:
- listing_action = 'insert'
- all dependent actions (listing_info, contact_info, images, hours) = 'insert'
- exit the lookup process for this row.
- If match is found in public → update bridge table with listing_internal_id.

⸻

2. Content Change Detection
- With listing_internal_id:
- Compare listing_content_hash → set listing_action = 'update' or 'skip'.
- Lookup listing_info using listing_internal_id + dmo_id:
- If not found → listing_info_action = 'insert' → dependent objects 'insert'.
- If found → compare listing_info_content_hash → set listing_info_action = 'update' or 'skip'.
- For dependent objects (contact_info, images, hours):
- If listing_info_action = 'insert' → all 'insert'.
- Else → compare hashes → set 'update' or 'skip'.

⸻

3. Duplicate Detection
- Within the staging table:
- use source_id as dedup key
- Flag rows with the same key:
- Keep one “primary” row for promotion.
- Set action = 'skip' for duplicates.
- Optionally, mark duplicates for human review (ok_to_promote = false).

⸻


## Key Concepts

### File Sizes, batch and splitting rules
**Assumptions**
- Average record size: 10 KB (high estimate)
-	DB insert batch: keep under 1–2 MB per batch for safety
-	Concurrent jobs: 3–5 workers at a time
-	Target memory use: ≤100 MB per worker

#### 1. DMO Import Batching / Splitting Guide

- Small DMOs (<500 listings): process in one go (~5 MB memory)
- Medium DMOs (500–5,000 listings): split into files ~500 records (~5 MB per file)
- Large DMOs (5,000–50,000 listings): split into files ~2,000–5,000 records (~20–50 MB per file), stream from disk/S3
- Always monitor memory per worker and DB transaction size
- Parallelism: 3–5 concurrent workers is safe; adjust based on worker memory

#### 2. Recommended File Sizes per Split File

- Small (≤500 listings): 1–5 MB per file
- Medium (500–5,000 listings): 5–10 MB per file
- Large (5,000–20,000 listings): 10–25 MB per file
- Very large (20,000–50,000 listings): 20–50 MB per file

#### 3. File Splitting Strategy (Dual Threshold)

- Limit file by maximum records OR maximum file size, whichever comes first
- Sequential file numbers: 001, 002, 003… for multiple files in a single import
- Store import metadata: import_id, record_count, file_size for monitoring and retries

#### 4. Streaming / Memory-Safe Processing Pattern

- Stream file from disk or S3
- Accumulate a batch (by records or file size)
- Process the batch records (save to file, process, insert to stage etc)
- Clear batch from memory
- Continue reading and processing batches until all records have been handled

#### 5. Example Listing JSON Size Estimates

- Average single listing (with 5-paragraph description, images, socials): 10–14 KB
- Gzipped: 2–3 KB
- 100 listings per file: ~1 MB
- 1,000 listings per file: ~10 MB
- 5,000 listings per file: ~50 MB (at 10 KB per listing)
- 5,000 listings per file: ~100 MB (at 20 KB per listing)

#### 6. Safe File Insert / Concurrency Guidelines

- Keep insert files ≤ 500–1,000 records for medium DMOs
- Use streaming for large DMOs to limit memory usage
- 3–5 concurrent workers per DMO is a safe default
- Monitor Postgres transaction size and memory per worker
- Smaller files are better for retries and failure isolation



### S3 File Naming Convention (Import ID + Sequential Number)
**originally I wanted to add country and state to file names, but it doesn't hold for objects that span geographys. And it seems that it adds clutter and isn't neccesary. Add this info to the files table and/or S3 metadata.**

#### raw file names

s3://{environment}-trippl-data/{stage}/{data-domain}/{dmo-id}/{date}/job-{job-id}-{file-seq}.{format}.gz

- job-id padded to 6 characters
- file-seq padded to 2 characters

#### Fields
  - environment: dev, staging, prod
  - stage: raw
  - data-domain: listings, reviews, events, etc.
  - dmo-id: your internal DMO ID (numeric or alphanumeric)
  - date: date of import in YYYYMMDD
  - import-id: timestamp or DMO+date combination that identifies the **entire import run**
  - file-seq: sequential number for files split from the same import (01, 02, 03…)
  - format: json, csv, xls
  - .gz: optional compression

- Notes:
  - All files with the same **import-id** belong to the same logical import run.
  - `file-seq` ensures unique filenames within a split import.
  - This approach eliminates confusion with multiple “batches” and keeps grouping and ordering simple.
  - geography will be stored in the db with pipeline.files and as metadata with files.  

#### image names
s3://{environment}-trippl-images/{dmo-id}/{listing-id}/{filename}.{ext}




### Public Hashes Generation
**public schema**

listings.listing_match_hash
- listings.name
- locations.address1
- locations.city

listings.listing_content_hash
- listings.name

listings.attributes_content_hash
- hash all bridge_attributes_listings.attribute_id fields with bridge_attributes_listings.listing_id = listings.id

listing_info.listing_info_content_hash
- listing_info.description

listing_info.images_content_hash
- hash all listing_images where listing_info_id = listing_info.id using fields image

locations.location_content_hash
- address1
- address2
- city
- state
- zip
- country
- lat
- lng

contact_info.contact_info_content_hash
- name
- email
- local_phone
- international_phone
- socials

opening_dates.opening_dates_content_hash
- hash all opening_dates records where listing_info_id = listing_info.id using fields start_date, end_date

opening_hours.opening_hours_content_hash
- hash all opening_hours records where opening_date_id = opening_dates.id where opening_dates.listing_info_id = listing_info.id using fields day_id, open_time, close_time


### Deterministic Internal IDs
- **Internal IDs are SERIAL IDs**: We use the database SERIAL primary keys (`public.listings.id`, `public.listing_info.id`, etc.) as our deterministic canonical identifiers.
- **Generation occurs during Promotion stage**: When data is promoted from stage to production, PostgreSQL automatically assigns SERIAL IDs which become our canonical internal IDs.
- Maps multiple source IDs (DMO, website, Priceline, GooglePlaceID) to a single internal `listing_id`.
- Maps one-to-one source IDs to a single internal `listing_info_id`.
- Mappings stored in `pipeline.bridge_source_target`.
- Ensures **consistent internal IDs** across multiple sources and imports.
- **Conflict Resolution Rules:**
  - **Listings level**: Multiple sources can map to the same `listing_id`
  - **Listing info level**: Only one source should map to a given `listing_info_id`
  - **Multiple listing_info per listing**: Allowed and expected, as long as IDs are unique.


### Deduplication
- **Stage table only:** `source_id` used for deduplication
- **Production:** dedupe_key recomputed on-the-fly for matching; not stored in production.


### Additional things for Cursor to implement directly
- Implement real geocoding API calls in Transform stage if lat/lng missing from raw and listing or listing_info match not found.  If match found then fill in lat/lng and missing address fields from production locations record.
- Idempotency: repeated promotions should not create duplicates
- Job queue workers: handle retry_count, locked_by, dequeue_attempts, next_attempt_at
- Admin UI: simple staging review table with promote button


# Stages details

## Configure
**Define pipeline behavior for a DMO and data source type**
- tab lists configs and lets you run a config to start a job
- dialog to create/update/delete pipeline.configs
- Most settings use default values. Allow overriding for setting:
- Field mapping rules (CSV column → DB column).
- Normalization rules (uppercase, trim, clean special chars).
- Deduplication settings (fields to hash, rounding precision for lat/lng).
- Promotion policies (merge_policy, update_bridge_table).

### Sample configure step transform field mappings
"transform": {
  "map_fields": {
    "listings.name": "Name",
    "listings.google_place_id": "GooglePlaceID",
    "listings.listing_status": "ListingStatus",
    "listings.listing_visible": "ListingVisible",
    "listing_info.description": "Description",
    "listing_info.info_visible": "InfoVisible",
    "listing_info.info_status": "InfoStatus",
    "contact_info.name": "ContactName",
    "contact_info.email": "Email",
    "contact_info.local_phone": "LocalPhone",
    "contact_info.international_phone": "InternationalPhone",
    "locations.address1": "Address1",
    "locations.address2": "Address2",
    "locations.city": "City",
    "locations.state": "State",
    "locations.zip": "Zip",
    "locations.country": "Country",
    "locations.lat": "Lat",
    "locations.lng": "Lng",
    "listing_images.image_1": "Image1",
    "listing_images.image_2": "Image2",
    "opening_dates.start_date_1": "StartDate1",
    "opening_dates.end_date_1": "EndDate1",
    "opening_dates.start_date_2": "StartDate2",
    "opening_dates.end_date_2": "EndDate2",
    "opening_hours.mon_open": "MonOpen",
    "opening_hours.mon_close": "MonClose",
    "bridge_attributes_listings.attribute_1": "Attribute1",
    "bridge_attributes_listings.attribute_2": "Attribute2"
  }
}


## Import
**Upload or import raw data**
-	Upload CSV, import from API, scrape from website.
- Save original as Raw File in raw area (audit trail).
-	Store metadata: import timestamp, user, source identifier ...

### Metadata added to raw import files
**the columns that we need added to the raw import file are:**
```
const metadataColumns = {
  'rec_num': index + 1,       // Sequential record number within file
  dmo_id: importConfig.dmoId, // DMO identifier
  source_id: sourceId,        // Unique source identifier
  imported_at: importedAt,    // Import timestamp
  imported_by: uploadedBy,    // User who imported (0 for pipeline system)
};
```

### source_id
- per-record unique identifier from the original source system (WordPress slug, Priceline ID, Foursquare ID, etc.)
- Can detect duplicates and support upserts
- Is deterministic (same record → same ID)
- Comes with the imported record; exists before Transform. It is used to check if this record already exists in your DB.
- an input record has one source_id, which can be used to lookup listings or listing_info
- In practice:
  - use source_id to lookup listing in DB.
    - not exist - stage new listing+listing_info.
    - exist
      - lookup listing_info using source_id
        - not exist - stage new listing_info
        - exist
          - check hashes
            - changes - stage for updates
            - no changes - stage for skipped

- one external record maps to a listing and a listing_info
- the same source_id is used for the listing and the listing_info
- for each external record, there will be a one possible listing_info match
- for each listing, there could be multiple matches to import records, from different dmos

#### what if the import type changes for the same source (dmo_id)
**e.g. BookingPEI**
source: csv      record: 1   hash: dmo_id+name+city  = 51

source: wp api   record: 1   hash: dmo_id+name+city  = 51

- so the determining factor for the source_id will be the dmo and listing details.
- it won't matter if we change the input type (csv, wordpress), the source_id should stay the same.
- that way we will be doing an update of an existing record, if we did a different import type.

#### how to generate source_id
| source        | source_id                                   |
|---------------|---------------------------------------------|
| Priceline     | id                                          |
| Foursquare    | id                                          |
| Google        | placeId                                     |
| csv           | dmo_id + normalize(name) + normalize(city)  |
| wordpress api | (dont need dmo_id?) source_url? or same hash as csv? |
| BPEI website  | 
| CC website    |
| PE website    |
| TPEI website  | 

```
// fields for normalizing: dmo_id, name, address, city, postal_code, source_url
const normalize = str =>
  str?.toLowerCase().trim().replace(/[^\w\s]/g, '') || '';

import crypto from 'crypto';
const sourceIdentifier = crypto
  .createHash('sha1')
  .update(`${source_id}|${normalize(name)}|${normalize(address)}|${normalize(city)}|${normalize(country)}`)
  .digest('hex');
```

#### field type
Need one field (source_id) that can store any kind of unique identifier, regardless of source — e.g.:
- ChIJN1t_tDeuEmsRUsoyG83frY4 (Google Place ID)
- H1234567890 (Priceline hotel ID)
- https://example.com/listing/123 (WordPress permalink) - or we use the same as csv, in case of changing import type, the records will still evaluate to the same source
- abc123xyz... (hash generated from CSV fields)

```sql
source_id TEXT NOT NULL
-- Can handle strings of any length (no need to specify a max).
-- PostgreSQL stores short strings efficiently (inline up to ~2kB).
-- Works for IDs, URLs, and hashes alike.
-- Lets you normalize across sources without worrying about type casting.

// unique across a single source (dmo_id) -- ie google, priceline, bookingpei.com. question: then what about reviews, same dmo_id?
CONSTRAINT unique_source_id_per_source UNIQUE (dmo_id, source_id)

- listing_info will have a single unique source_id
- but listings can have multiple source_ids, which are mapped in the bridge table by dmo_id
```
#### mapping the source_id to listings and listing_info
- there is one source_id for an import record. so within an import job, listing and listing_info will have the same source_id.
- Use one shared bridge table (source_mappings) with a target_type column

```sql
CREATE TABLE pipeline.bridge_source_target (
  id SERIAL PRIMARY KEY,
  dmo_id INT NOT NULL,              -- distinguishes different sources (ie, google and priceline could have a listing with the same id)
  source_id TEXT NOT NULL,          -- e.g. 'ChIJN1t_tDeuEmsRUsoyG83frY4'
  target_type TEXT NOT NULL,        -- e.g. 'listing', 'listing_info'
  target_id INT NOT NULL,           -- internal DB record ID
  UNIQUE (dmo_id, source_id, target_type)
);

Example Data:

dmo_id      source_id                     target_type     target_id
200 (PL)    ChIJN1t_tDeuEmsRUsoyG83frY4   listing         101
200 (PL)    ChIJN1t_tDeuEmsRUsoyG83frY4   listing_info    201
1 (google)  ChIJN1t_tDeuEmsRUsoyG83frY4   listing         60
5 (bpei)    xxx                           listing         101
5 (bpei)    xxx                           listing_info    500

-- FIND THE DB RECORD
SELECT target_type, target_id
FROM pipeline.bridge_source_target
WHERE dmo_id = 200
  AND source_id = 'ChIJN1t_tDeuEmsRUsoyG83frY4';
```

                      +---------------------------+
                      | bridge_source_target      |
                      |---------------------------|
                      | id (PK)                   |  <-- primary key index
                      | dmo_id                    |
                      | source_id                 |
                      | target_id                 |
                      | target_type               |
                      | certainty_score           |
                      | mapped_at                 |
                      +---------------------------+
                               ^
                               |
                               |
          ----------------------------------------------
          |                                            |
          |                                            |
   1) Deduplication check                         2) Lookup by target_id
   (Check if source record already exists)        (Find internal record mappings)
          |                                            |
  SELECT * FROM bridge_source_target           SELECT source_id, dmo_id
  WHERE dmo_id = ?                              FROM bridge_source_target
    AND source_id = ?                             WHERE target_id = ?
    AND target_type = ?                                AND target_type = ?



---

## Validation
**Purpose: Validate transformed data and prepare for staging.**
- Key Details:
- Data type validation (lat/lng bounds, email format, phone format, etc.).
- Required field checks based on DMO configuration.
- Business rule validation (DMO-specific constraints, geographic bounds).
- Data quality scoring (optional for Phase 2).
- Fail fast for critical errors, collect warnings for optional issues.
	



**find listing_info**
- NOTE: we never really have to lookup listing_info by it's content. Once we have found the listing, and we have the import dmo_id, then a listing_info record exists for that listing and dmo_id (check for content changes), or it doesn't (create new).
- todo: figure out if we need to do a verification with a listings_info_match_hash.
- todo: also figure out how to handle duplicate records that may have different information: which is used for the update?  I guess we add to the staging table as duplicates and force checking which one is the updating record source.
- 
- if new listing record, then also new listing_info record
- (there really shouldn't be any match in the db, should we double check to catch unexpected results?)
- if listing_id was found
- listing_info_id = select target_id from bridge_source_target(source_id, source_type='listing_info', dmo_id=import_dmo_id) (this is where parent id could be used instead of dmo_id to make sure we have the right listing_info)
- if not found, then we make sure we are not just missing the mapping, lookup listing_info by listing_id and dmo_id.
- if not found (which it shouldn't be)
- create new listing_info

## dedupe_key in staging
- Exists only in staging tables (you don’t store it in production).
- It’s generated on-the-fly during the transform step, usually by hashing a combination of normalized fields, e.g.:  hash(name_normalized + address_normalized + lat_rounded + lng_rounded)	
- If a match is found:
- Conflict resolution is triggered (e.g., latest_wins, field_level_merge, skip_if_exists).
- If no match is found:
- Stage row is inserted as a new row in production.


⸻


## Admin pages
add permissions for new pipeline functions. add all permissions to the super_admin role.

all Pipeline menus and pages should be protected on the front and back end with required permission pipeline:read.

make a new sidebar menu section called Pipeline, visible to users with permission

make admin pages to manage all steps of the pipeline. 

admin pages should follow the structure of existing admin pages (topbar, sidebar etc)


## Job Scheduling
use manual scheduling/execution on localhost.

on dev/staging/app use aws SQS to schedule and execute jobs in the job_queue

**if dmo config changes** then add a dialog at save time that checks for scheduled jobs and lets user ignore or cancel.  Changes to schedule shouldn't be an issue, though an old job with old config might overwrite new job(s).


## suggested Pipeline Schema table design

```sql

example config_params:
"import": {
  "source_type": "csv",   -- 'csv', 'wordpress', 'drupal', 'website', 'api', 'eventbrite'
  "file_format": "csv",   -- 'csv', 'json'
  "delimiter": ",",
  "encoding": "utf-8",
  "skip_header": true
}

// do the field mapping in the transform step, not the load-to-staging step.
// That way, your staging table always looks like your internal canonical schema (name, address, lat, lng, etc.), regardless of source.

"transform": {
  "map_fields": {
    "listings.name": "Name",
    "locations.address1": "Street Address",
    "contact_info.email": "Email"
  },
  "normalize_fields": [
    "listings.name",
    "locations.address1",
    "listings.category"
  ],
  "round_lat_lng": 5,
  "hash_fields": ["listings.name", "locations.address1", "lat", "lng", "listings.category"],
  "dedupe_key_fields": ["listings.name", "locations.address1", "lat", "lng"]
}

"validation": {
  "data_types": {
    "locations.lat": "numeric",
    "locations.lng": "numeric",
    "contact_info.email": "email",
    "contact_info.local_phone": "phone"
  },
  "required_fields": [
    "listings.name",
    "locations.address1",
    "locations.city"
  ],
  "business_rules": {
    "lat_bounds": [-90, 90],
    "lng_bounds": [-180, 180],
    "dmo_geographic_bounds": {
      "min_lat": 40.0,
      "max_lat": 45.0,
      "min_lng": -80.0,
      "max_lng": -70.0
    }
  },
  "quality_scoring": {
    "enabled": true,
    "weights": {
      "completeness": 0.4,
      "accuracy": 0.3,
      "consistency": 0.3
    }
  }
}

"staging": {
  "match_on": [
    "listings.dedupe_key",
    "locations.dedupe_key",
    "contact_info.dedupe_key"
  ],
  "conflict_resolution": "update_if_changed",
  "status_field": {
    "listings": "ok_to_promote",
    "locations": "ok_to_promote",
    "contact_info": "ok_to_promote"
  },
  "audit_fields": ["created_at", "updated_at", "imported_from"]
}

"promotion": {
  "merge_policy": "latest_wins", // if a row exists then staging version overwrites it
  // "merge_policy": "skip_if_exists" // keep production data if it already exists
  // "merge_policy": "field_level_merge" // update only fields that changed
  "update_bridge_table": true  // using a bridge table for many-to-many relationships, set to true to ensure relationships are updated during promotion. If false, only the main tables (listings, locations) are updated, but bridge/assocaiation tables are left stale (?)
  "audit_fields": ["promoted_at", "promoted_by", "imported_from"],
  "conflict_resolution": {
    "listings": "latest_wins",
    "locations": "field_level_merge",
    "contact_info": "latest_wins"
  }
}

    <!-- ******************** -->
      <!-- PIPELINE SCHEMA      -->
      <!-- ******************** -->

      -- Create pipeline schema
      CREATE SCHEMA IF NOT EXISTS pipeline;

      -- Create enum type
      CREATE TYPE pipeline.target_type AS ENUM ('listings', 'listing_info');

      -- Bridge Source Target table (lookup only)
      CREATE TABLE pipeline.bridge_source_target (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        source_id TEXT NOT NULL,     -- like Priceline id, or source url, or computed (hash(dmo_id+name+city))
        target_id INT,               -- id of listings or listing_info record
        target_type pipeline.target_type NOT NULL,   -- e.g. 'listings', 'listing_info'
        certainty_score NUMERIC(5,4) DEFAULT 1.0,    -- confidence score for the match
        mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT unique_dmo_source_target UNIQUE (dmo_id, source_id, target_type)
      );

      -- Import Configs table
      CREATE TABLE pipeline.configs (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        name TEXT NOT NULL,

        data_domain TEXT NOT NULL, -- listings, events, reviews
        source_type TEXT NOT NULL, -- csv, wordpress, drupal, website, api, eventbrite
        source_url TEXT,
        schedule TEXT,
        config JSONB NOT NULL,
        field_mappings JSONB NOT NULL,
        validation_rules JSONB NOT NULL,
        active BOOLEAN NOT NULL DEFAULT TRUE,
        version INT DEFAULT 1,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT unique_dmo_source_name UNIQUE (dmo_id, name)
      );

      -- Create enum type
      CREATE TYPE pipeline.pipeline_stage AS ENUM ('import', 'transform', 'validate', 'stage', 'review', 'promote');

      -- Jobs table
      CREATE TABLE pipeline.jobs (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        current_stage pipeline.pipeline_stage NOT NULL,
        status TEXT DEFAULT 'pending', -- pending, running, completed, failed, cancelled

        triggered_by TEXT,
        trigger_type TEXT, -- e.g., manual / schedule / system
        records_total INT DEFAULT 0,
        records_new INT DEFAULT 0,
        records_updated INT DEFAULT 0,
        records_deleted INT DEFAULT 0,
        records_duplicates INT DEFAULT 0,
        records_unchanged INT DEFAULT 0,
        records_failed INT DEFAULT 0,
        records_warning INT DEFAULT 0,

        enqueued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        started_at TIMESTAMP,
        finished_at TIMESTAMP
      );

      -- Job Stages table
      CREATE TABLE pipeline.job_stages (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        name pipeline.pipeline_stage NOT NULL,
        order_index INT NOT NULL,
        status TEXT DEFAULT 'pending', -- pending, running, completed, failed, cancelled

        attempt_count INT DEFAULT 0, -- current attempt number
        retry_count INT DEFAULT 0, -- number of times the stage has been retried
        next_attempt_at TIMESTAMP, -- next time the stage will be attempted
        locked_by TEXT, -- user/worker who locked the stage
        locked_at TIMESTAMP, -- time the stage was locked

        message TEXT,
        result_summary JSONB,

        enqueued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        started_at TIMESTAMP,
        finished_at TIMESTAMP,
        CONSTRAINT unique_job_stage UNIQUE (job_id, name)
      );

      -- Config Snapshots table
      CREATE TABLE pipeline.config_snapshots (
        id SERIAL PRIMARY KEY,
        configs_id INT NOT NULL REFERENCES pipeline.configs(id), -- used to lookup active jobs for a given config
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        name TEXT NOT NULL,

        data_domain TEXT NOT NULL, -- listings, events, reviews
        source_type TEXT NOT NULL, -- csv, wordpress, drupal, website, api, eventbrite
        source_url TEXT,
        schedule TEXT,
        config JSONB NOT NULL,
        field_mappings JSONB NOT NULL,
        validation_rules JSONB NOT NULL,
        version INT DEFAULT 1,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

      -- Files table
      CREATE TABLE pipeline.files (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NULL REFERENCES pipeline.jobs(id),
        stage_name TEXT NOT NULL, -- import, transform, validate

        file_path TEXT NOT NULL,
        import_id TEXT NOT NULL,
        file_seq INT NOT NULL,
        data_domain TEXT NOT NULL, -- listings, events, reviews
        country VARCHAR(2),
        state VARCHAR(2),
        file_date DATE,

        batch INT GENERATED ALWAYS AS IDENTITY,  -- for sequential numeric batch IDs
        format TEXT,
        file_size BIGINT,
        record_count INT,
        file_hash VARCHAR(32),

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

     -- Job Results table: summary of results by stage
      CREATE TABLE pipeline.stage_results (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        job_stages_id INT NOT NULL REFERENCES pipeline.job_stages(id),

        -- Lifecycle and outcome
        status TEXT NOT NULL, -- current progress or outcome: pending, running, completed, failed,cancelled

        records_total INT DEFAULT 0,
        records_new INT DEFAULT 0,
        records_updated INT DEFAULT 0,
        records_deleted INT DEFAULT 0,
        records_duplicates INT DEFAULT 0,
        records_unchanged INT DEFAULT 0,
        records_failed INT DEFAULT 0,
        records_warning INT DEFAULT 0,

        -- Timestamps - stage specific
        started_at TIMESTAMP NULL,
        finished_at TIMESTAMP NULL
      );

      -- Job Errors table: detailed errors by record
      --   jobs can complete but still have errors. Errors don't mean a job/stage failed. Failed is a bigger deal.
      CREATE TABLE pipeline.job_errors (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        job_stages_id INT NOT NULL REFERENCES pipeline.job_stages(id),

        file_id INT REFERENCES pipeline.files(id),
        record_id VARCHAR,
        original_record JSONB,
        error_type TEXT,
        error_message TEXT,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

      /*
      ** ------------------------------------------
      ** Indexes
      ** ------------------------------------------
      */

      CREATE INDEX idx_pipeline_jobs_dmo_id ON pipeline.jobs(dmo_id);
      CREATE INDEX idx_pipeline_job_stages_job_id ON pipeline.job_stages(job_id);
      CREATE INDEX idx_pipeline_files_job_id ON pipeline.files(job_id);
      -- lookup listings by source_id and target_type
      CREATE INDEX idx_bridge_source ON pipeline.bridge_source_target(source_id, target_type);
      -- lookup listing_info by source_id, target_type and dmo_id
      -- already covered by the bridge_source_target table CONSTRAINT
```

## Suggesting Staging Schema

**Schema name:** staging
-	Stores all imported/transformed rows before promotion.
-	Keeps full audit trail, dedupe keys, hashes, and flags.
-	All raw fields are preserved, along with normalized/hashed versions for deduplication.

```sql
     -- Create stage schema
      CREATE SCHEMA IF NOT EXISTS stage;

      -- Listings Flat Table (consolidated stage table)
      CREATE TABLE stage.listings_flat (
        -- Primary key for stage row
        id SERIAL NOT NULL PRIMARY KEY,

        -- Source information
        dmo_id INTEGER NOT NULL,
        source_id TEXT NOT NULL, -- like Priceline id, or computed from (dmo_id or source_url) + source_identifier (slug?)
        source_url TEXT, -- optional source url or source id base
        source_identifier TEXT, -- optional source identifier (priceline id, slug?)
        -- Timestamps for auditing
        created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW(),

        -- Internal IDs linking to public tables (may be NULL if new record)
        listing_internal_id INTEGER,

        -- Hashes for matching (and deduplication)
        listing_match_hash CHAR(32),
        -- no need for listing_info_match_hash because there is only one listing_info per listing-dmo_id

        -- Hashes for change detection
        listing_content_hash CHAR(32),
        location_content_hash CHAR(32),
        attributes_content_hash CHAR(32),

        listing_info_content_hash CHAR(32),
        contact_info_content_hash CHAR(32),
        images_content_hash CHAR(32),
        opening_dates_content_hash CHAR(32),
        opening_hours_content_hash CHAR(32),

        -- Overall promotion/action status
        promote_status VARCHAR DEFAULT 'pending', -- pending, skipped, succeeded, failed
        quality_score NUMERIC(5,2),
        manual_review_required BOOLEAN DEFAULT FALSE,

        -- Validation and quality control
        validation_errors TEXT,

        listing_action VARCHAR(16) DEFAULT 'pending', -- pending, skip, insert, update, delete, error
        listing_info_action VARCHAR(16) DEFAULT 'pending',
        location_action VARCHAR(16) DEFAULT 'pending',
        contact_info_action VARCHAR(16) DEFAULT 'pending',
        images_action VARCHAR(16) DEFAULT 'pending',
        opening_dates_action VARCHAR(16) DEFAULT 'pending',
        opening_hours_action VARCHAR(16) DEFAULT 'pending',
        attributes_action VARCHAR(16) DEFAULT 'pending',

        -- Raw content fields (optional, depending on pipeline needs)
        listing_name VARCHAR,
        listing_google_place_id VARCHAR,
        listing_info_description TEXT,
        loc_address1 VARCHAR,
        loc_address2 VARCHAR,
        loc_region VARCHAR,
        loc_city VARCHAR,
        loc_state VARCHAR,
        loc_zip VARCHAR,
        loc_country CHAR(2),
        loc_lat NUMERIC(10,8),
        loc_lng NUMERIC(11,8),
        ci_name VARCHAR,
        ci_email VARCHAR,
        ci_local_phone VARCHAR,
        ci_international_phone VARCHAR,
        ci_created_at TIMESTAMP WITHOUT TIME ZONE,
        ci_updated_at TIMESTAMP WITHOUT TIME ZONE,
        ci_website_url VARCHAR,
        ci_socials JSONB,
        images JSONB,
        dates JSONB,
        hours JSONB,
        attributes JSONB
      );

      -- Create indexes for performance

      -- 1. Ensure unique stage row per source
      CREATE UNIQUE INDEX idx_stage_source_dmo ON stage.listings_flat(dmo_id, source_id);

      -- 2. Lookups for linking to production internal IDs
      CREATE INDEX idx_stage_listing_internal ON stage.listings_flat(listing_internal_id);

      -- 3. Deduplication / match hash lookups
      CREATE INDEX idx_stage_listing_match_hash ON stage.listings_flat(listing_match_hash);
      CREATE INDEX idx_stage_listing_content_hash ON stage.listings_flat (listing_content_hash);
      CREATE INDEX idx_stage_location_content_hash ON stage.listings_flat (location_content_hash);
      CREATE INDEX idx_stage_listing_info_content_hash ON stage.listings_flat (listing_info_content_hash);
      CREATE INDEX idx_stage_contact_info_content_hash ON stage.listings_flat (contact_info_content_hash);
      CREATE INDEX idx_stage_images_content_hash ON stage.listings_flat (images_content_hash);
      CREATE INDEX idx_stage_dates_content_hash ON stage.listings_flat (opening_dates_content_hash);
      CREATE INDEX idx_stage_hours_content_hash ON stage.listings_flat (opening_hours_content_hash);
      CREATE INDEX idx_stage_attributes_content_hash ON stage.listings_flat (attributes_content_hash);

      -- 4. Promotion / review queries
      CREATE INDEX idx_stage_promote_status ON stage.listings_flat(promote_status);
      CREATE INDEX idx_stage_manual_review ON stage.listings_flat(manual_review_required);

```

**Notes on staging:**
-	dedupe_key is always computed using a hash of normalized key fields (e.g., name + address + lat/lng).
-	ok_to_promote is false by default, set to true only after review.
-	All imported fields are kept to allow audit and traceability.

## Suggested Public Schema Updates
```sql
     <!-- ****************************** -->
      <!-- PUBLIC SCHEMA HASH FIELDS -->
      <!-- ****************************** -->

      -- Add hash fields to public schema tables for pipeline tracking

      -- Listings table hash fields
      ALTER TABLE public.listings
      ADD COLUMN listing_match_hash CHAR(32),
      ADD COLUMN listing_content_hash CHAR(32),
      ADD COLUMN attributes_content_hash CHAR(32);

      -- Listing Info table hash fields
      ALTER TABLE public.listing_info
      ADD COLUMN listing_info_content_hash CHAR(32),
      ADD COLUMN images_content_hash CHAR(32);

      -- Locations table hash fields
      ALTER TABLE public.locations
      ADD COLUMN location_content_hash CHAR(32);

      -- Contact Info table hash fields
      ALTER TABLE public.contact_info
      ADD COLUMN contact_info_content_hash CHAR(32);

      -- Opening Dates table hash fields
      ALTER TABLE public.opening_dates
      ADD COLUMN opening_dates_content_hash CHAR(32);

      -- Opening Hours table hash fields
      ALTER TABLE public.opening_hours
      ADD COLUMN opening_hours_content_hash CHAR(32);

```
**Notes on production public schema:**
-	Only reviewed and promoted rows live here.
-	Deduplication is applied during promotion; no dedupe keys stored unless needed for performance.
-	promoted_at and promoted_by track the exact promotion event.
-	manually_modified flags ensure pipeline doesn’t overwrite manual edits.

✅ Key Design Points for Enterprise-Grade Traceability
1.	Staging = full audit, full imported data, dedupe & normalized fields
2.	Production = clean, promoted data, audit of promotion + manual edits
3.	Multi-table mapping is explicit: listings.name, contact_info.email, locations.address1
4.	Deduplication & hashing: staging generates dedupe_key per record; production dedupe is computed on-the-fly during promotion
5.	Audit fields everywhere: imported_from, created_at, updated_at, promoted_at, promoted_by
6.	Bridge tables follow the same pattern for relational mapping, with promotion/audit fields


# Step by step parent/sub-table deduplication, hashing, promotion, and matching logic. 

# Listings Pipeline: Parent and Sub-Table Promotion

## 1. Parent Tables

| Table | Key Fields / Dedupe Key | Promotion Flag |
|-------|------------------------|----------------|
| `listings` | `name + address + lat/lng` | `ok_to_promote` |
| `listing_info` | `description + contact_info_id` | `ok_to_promote` |

- `dedupe_key` is computed in staging from key identifying fields.  
- Only rows with `ok_to_promote = true` are eligible for promotion.  
- Matching during promotion is done against production tables using `dedupe_key`.

---

## 2. Sub-Tables

Promotion of sub-tables is **conditional on parent approval**. Each sub-table has its own dedupe key used for matching against production.

| Sub-Table | Key Fields for Hash / Dedupe | Linked To | Notes on Matching / Promotion |
|-----------|-----------------------------|-----------|-------------------------------|
| `contact_info` | `email + phone + name` | `listing_info.contact_info_id` | Reuse existing row in production if dedupe key matches; else insert new. |
| `locations` | `address1 + city + state + zip + country` | `listings.location_id` | Same: match dedupe key, reuse or insert new. |
| `listing_images` | `listing_info_id + image_url` | `listing_info.id` | Avoid duplicate images per listing. |
| `opening_dates` | `listing_info_id + start_date + end_date` | `listing_info.id` | Prevent duplicate date ranges for same listing_info. |
| `opening_hours` | `opening_date_id + day_id + open_time + close_time` | `opening_dates.id` | Match exact schedule; parent opening_date must exist. |
| `bridge_attributes_listings` | `listing_id + attribute_id` | `listings.id` | Deduplicate the pair of foreign keys; parent listing must be promoted. |





