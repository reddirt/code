# Listings Pipeline Design
**goal:** implement an enterprise grade listings pipeline with a powerful graphical interface for managing data pipeline.

## Pipeline Stages

1. **Configure** ‚Äì Set pipeline rules for a DMO.
2. **Import** ‚Äì Upload CSV or import .json, store as Raw File.
3. **Transform** ‚Äì Map, normalize, hash, dedupe, and assign **internal canonical IDs** before staging.
4. **Validate** ‚Äì Data type validation, required field checks, business rules, and data quality scoring.
5. **Stage** ‚Äì Load validated data to staging schema for review.
6. **Review** ‚Äì Manual verification; resolve low certainty or conflicts, set `ok_to_promote`.
7. **Promote** ‚Äì Move approved rows to production; sub-tables promote conditionally based on parent.

---

## Key Concepts

### Source_id
- A unique identifier from the original source system (WordPress slug, Priceline ID, Foursquare ID, etc.)
- Comes with the imported record; exists before Transform. It is used to check if this record already exists in your DB.
- In practice: lookup source_id in DB
	- If found ‚Üí you already have the internal ID ‚Üí just update hashes, etc.
	- If not ‚Üí compute a new Deterministic Internal ID using normalized key fields.

### Deterministic Internal IDs
- Computed during **Transform stage** after field mapping, normalization, rounding lat/lng, and hashing; before staging. Used to identify the canonical listing internally.
- Our own internal ID for a listing, computed deterministically from key fields (name, address, lat/lng, etc.)
- Maps multiple source IDs (DMO, website, Priceline, GooglePlaceID) to a single internal `listing_id`.
- Stored in `pipeline.bridge_source_listing` and `pipeline.bridge_source_listinginfo`.
- Ensures **consistent internal IDs** across multiple sources and imports.
- **Conflict Resolution Rules:**
  - **Listings level**: Multiple sources can map to the same `listing_id` (tracked in `bridge_source_listing`). Conflicts are expected and allowed.
  - **Listing info level**: Only one source should map to a given `listing_info_id` (enforced via `bridge_source_listinginfo`). Conflicts must be flagged and resolved.
  - **Multiple listing_info per listing**: Allowed and expected, as long as IDs are unique.

### Deduplication
- **Staging only:** `dedupe_key` generated per table via hash of normalized fields (e.g., `name + address + lat/lng`).
- **Production:** dedupe_key recomputed on-the-fly for matching; not stored in production.
- Sub-tables (`contact_info`, `locations`, `listing_images`, `opening_dates`, `opening_hours`, bridge tables) use parent-driven promotion and their own dedupe keys.

---

## Pipeline Stages (Implementation Summary)

| Step | Purpose / Action | Key Fields / Flags | Notes / Implementation Details |
|------|-----------------|-----------------|--------------------------------|
| **1. Configure** | Define pipeline behavior for a specific DMO | `field_mappings`, `normalize_rules`, `dedupe_fields`, `merge_policy`, `update_bridge_table` | Store pipeline settings per DMO; drives all downstream steps including transform, staging, and promotion. Determines how CSV/API fields map to internal tables. |
| **2. Import** | Load raw data | `raw_file_path`, `imported_from`, `imported_at`, `uploaded_by`, `dmo_id`, `source_location`, `recorded_at` | Upload CSV, JSON, or API data. Save original files to preserve audit trail. Minimal validation occurs at this stage. |
| **3. Transform** | Clean, normalize, map, dedupe, and assign internal IDs | `map_fields`, `normalize_fields`, `round_lat_lng`, `hash_fields`, `dedupe_key`, `listing_id` | Apply CSV/API ‚Üí DB column mapping. Normalize strings, round lat/lng, compute hashes for deduplication. Compute **deterministic internal IDs** for listings and listing_info to decouple from external IDs (GooglePlaceID, DMO ID, Priceline, etc.). Output transformed file ready for validation. |
| **4. Validate** | Data type validation, required field checks, business rules | `validation_rules`, `data_types`, `required_fields`, `business_rules`, `quality_score` | Validate data types, check required fields, apply business rules (e.g., lat/lng within bounds), compute data quality scores. Fail fast for critical errors, collect warnings for optional issues. Output validated file ready for staging. |
| **5. Stage** | Load validated data into staging schema | `staging_table_rows`, `dedupe_key`, `imported_from`, `created_at`, `updated_at`, `ok_to_promote`, `audit_fields` | Insert all validated rows from validated file. Detect duplicates within the batch. Set `ok_to_promote` flags based on validation results. Keep full audit trail. Sub-tables (contact_info, locations, images, opening_dates, opening_hours, bridge tables) get staged and linked to parent IDs. |
| **6. Review** | Manual verification before promotion | `ok_to_promote`, `conflict_flags`, `add/update/delete indicators` | Show staging rows grouped by dedupe_key. Highlight changes: additions, updates, deletions. Allow manual approval. Resolve intra-stage conflicts. Only rows marked `ok_to_promote = true` are eligible for promotion. |
| **7. Promote** | Move approved rows to production | `merge_policy`, `update_bridge_table`, `promoted_at`, `promoted_by` | Compare dedupe_key with production rows on-the-fly. Insert new rows or update existing rows according to merge_policy. Respect manually_modified flags to avoid overwriting manual edits. Update bridge tables if needed. Record audit: promoted_at, promoted_by, imported_from. |

### Pipeline Stages additional information

1. Configure
	‚Ä¢	Purpose: Define pipeline behavior for a specific DMO (data source).
	‚Ä¢	Key Details:
	‚Ä¢	Source info (name, file type, API credentials).
	‚Ä¢	Field mapping rules (CSV column ‚Üí DB column).
	‚Ä¢	Normalization rules (uppercase, trim, clean special chars).
	‚Ä¢	Deduplication settings (fields to hash, rounding precision for lat/lng).
	‚Ä¢	Promotion policies (merge_policy, update_bridge_table).

‚∏ª

2. Import
	‚Ä¢	Purpose: Ingest raw data.
	‚Ä¢	Key Details:
	‚Ä¢	Upload CSV, JSON, or API data.
	‚Ä¢	Save original as Raw File in staging area (audit trail).
	‚Ä¢	Store metadata: import timestamp, user, source identifier.
	‚Ä¢	Optionally validate basic CSV structure before processing.

‚∏ª

3. Transform
	‚Ä¢	Purpose: Clean, normalize, and prepare data for validation.
	‚Ä¢	Key Details:
	‚Ä¢	Map source columns ‚Üí DB fields.
	‚Ä¢	Normalize fields (trim, lowercase, remove special chars).
	‚Ä¢   Check source_id in public DB ‚Üí
   		‚îú‚îÄ If found ‚Üí copy lat/lng & address
   		‚îî‚îÄ If not found ‚Üí geocode ‚Üí assign lat/lng & address ‚Üí assign new internal ID
	‚Ä¢	Round lat/lng if used in dedupe.
	‚Ä¢	Compute hash fields for dedupe.
	‚Ä¢	Generate staging dedupe_key.
	‚Ä¢	Produce Transformed File ready for validation.

‚∏ª

4. Validate
	‚Ä¢	Purpose: Validate transformed data and prepare for staging.
	‚Ä¢	Key Details:
	‚Ä¢	Data type validation (lat/lng bounds, email format, phone format, etc.).
	‚Ä¢	Required field checks based on DMO configuration.
	‚Ä¢	Business rule validation (DMO-specific constraints, geographic bounds).
	‚Ä¢	Data quality scoring (optional for Phase 2).
	‚Ä¢	Fail fast for critical errors, collect warnings for optional issues.
	‚Ä¢	Output Validated File ready for staging load.
	‚Ä¢	Validation results stored in pipeline.job_errors for failed records.

‚∏ª

5. Stage
	‚Ä¢	Purpose: Load validated data into staging schema for review.
	‚Ä¢	Key Details:
	‚Ä¢	Insert all validated records from validated file into staging tables.
	‚Ä¢	Compute staging-only fields (dedupe_key, imported_from, audit fields).
	‚Ä¢	Detect duplicates within staging and optionally mark conflicts.
	‚Ä¢	Set initial promotion flags/status (ok_to_promote = false if duplicate/missing info).
	‚Ä¢	Keep all rows for full audit/review purposes.

‚∏ª

6. Review
	‚Ä¢	Purpose: Manual verification before promotion.
	‚Ä¢	Key Details:
	‚Ä¢	Display staging rows grouped by dedupe_key.
	‚Ä¢	Highlight: additions, updates, potential deletions.
	‚Ä¢	Allow user to mark rows ok_to_promote or leave blocked.
	‚Ä¢	Resolve intra-stage conflicts manually if needed.
	‚Ä¢	Once verified, click "Promote" to trigger promotion stage.

‚∏ª

7. Promote
	‚Ä¢	Purpose: Move verified staging data to production (public schema).
	‚Ä¢	Key Details:
	‚Ä¢	Compare dedupe_key with production rows on-the-fly.
	‚Ä¢	Insert new rows or update existing rows according to merge_policy.
	‚Ä¢	Respect manually_modified flags to avoid overwriting manual edits.
	‚Ä¢	Update bridge tables (e.g., attributes relationships) if needed.
	‚Ä¢	Record audit: promoted_at, promoted_by, imported_from.
	‚Ä¢	Optional cleanup or archiving of staging batch.


### Additional things for Cursor to implement directly
- Implement real geocoding API calls in Transform stage if lat/lng missing
- Implement validation stage with data type checks, required field validation, and business rules
- Save validated data to validated file for staging consumption
- Ensure staging.status field is respected during manual review
- Idempotency: repeated promotions should not create duplicates
- Job queue workers: handle retry_count, locked_by, dequeue_attempts, next_attempt_at
- Admin UI: simple staging review table with promote button
- Only a minimal subset of listing fields needs to be handled for MVP (name, address, lat, lng)

‚∏ª

# Stages details

## Import stage
### Sample import file csv file headers (or .json file fields)
SourceId,Name,AlternateNames,NameCode,Description,Category,SubCategories,
Region,Address1,Address2,City,State,Zip,Country,Lat,Lng,Timezone,UtcOffset,Elevation,
ContactName,Email,LocalPhone,InternationalPhone,
Website,Facebook,Instagram,Youtube,Tiktok,Twitter,Pinterest,TripAdvisor,Yelp,
Images,
Languages,Rating,Reviews,Price,Amenities,Seasons,Hours,OpenedDate,ClosedDate,Other

**Notes**
-	Each row corresponds to one listing.
-	Multi-value fields (images, opening dates, attributes) are repeated horizontally (Image1, Image2, etc.).
-	Empty cells are allowed; the transform step can ignore or fill defaults.
-	No table prefixes are needed ‚Äî the transform configuration handles mapping CSV columns to tables/fields.


### Sample transform field mappings

"transform": {
  "map_fields": {
    "listings.name": "Name",
    "listings.google_place_id": "GooglePlaceID",
    "listings.listing_status": "ListingStatus",
    "listings.listing_visible": "ListingVisible",
    "listing_info.description": "Description",
    "listing_info.info_visible": "InfoVisible",
    "listing_info.info_status": "InfoStatus",
    "contact_info.name": "ContactName",
    "contact_info.email": "Email",
    "contact_info.local_phone": "LocalPhone",
    "contact_info.international_phone": "InternationalPhone",
    "locations.address1": "Address1",
    "locations.address2": "Address2",
    "locations.city": "City",
    "locations.state": "State",
    "locations.zip": "Zip",
    "locations.country": "Country",
    "locations.lat": "Lat",
    "locations.lng": "Lng",
    "listing_images.image_1": "Image1",
    "listing_images.image_2": "Image2",
    "opening_dates.start_date_1": "StartDate1",
    "opening_dates.end_date_1": "EndDate1",
    "opening_dates.start_date_2": "StartDate2",
    "opening_dates.end_date_2": "EndDate2",
    "opening_hours.mon_open": "MonOpen",
    "opening_hours.mon_close": "MonClose",
    "bridge_attributes_listings.attribute_1": "Attribute1",
    "bridge_attributes_listings.attribute_2": "Attribute2"
  }
}


## Transform stage

### Transform Stage Highlights

Purpose of the Transform Stage

The transform stage is the ‚Äúdata wrangling‚Äù step:
	‚Ä¢	Converts raw imported data into a consistent, normalized, and structured format.
	‚Ä¢	Prepares fields for deduplication, hashing, and staging insertion.
	‚Ä¢	Ensures that the mapping from CSV/API columns ‚Üí database columns is applied.
	‚Ä¢	Applies optional normalization and rounding for comparison and hash keys.

1. **Field Mapping** ‚Äì CSV/API ‚Üí DB tables (multi-table).
2. **Normalization** ‚Äì Trim, lowercase, remove special chars.
3. **Lat/Lng Rounding** ‚Äì For dedupe consistency.
4. **Hash Fields & Dedupe Key** ‚Äì Generate staging `dedupe_key`.
5. **Deterministic Internal IDs** ‚Äì Map multiple external IDs to `listing_id`.
6. **Output** ‚Äì Transformed file ready for staging load.

### Key Sub-Steps in Transform

a) Field Mapping
	‚Ä¢	Maps source columns to database columns.
	‚Ä¢   Example

"map_fields": {
  "listings.name": "Name",
  "locations.address1": "Street Address",
  "contact_info.email": "Email"
}

	‚Ä¢	Effect:
	‚Ä¢	Name in CSV becomes listings.name in staging.
	‚Ä¢	Ensures all tables are properly populated for multi-table imports.

‚∏ª

b) Normalization
	‚Ä¢	Cleans and standardizes fields:
	‚Ä¢	Lowercase / uppercase names or emails
	‚Ä¢	Remove extra whitespace
	‚Ä¢	Strip special characters
	‚Ä¢	Example:

"normalize_fields": ["listings.name", "locations.address1"]

	‚Ä¢	Effect:
	‚Ä¢	Makes deduplication and hashing consistent.
	‚Ä¢	Reduces false duplicates caused by formatting differences.

‚∏ª

c) Lat/Lng Rounding
	‚Ä¢	Round coordinates for deduplication:

	"round_lat_lng": 5

		‚Ä¢	Effect:
	‚Ä¢	lat/lng rounded to 5 decimal places.
	‚Ä¢	Used in dedupe key and hash calculation.
	‚Ä¢	Original lat/lng preserved for mapping and production.

‚∏ª

d) Hash Fields
	‚Ä¢	Compute hash of selected fields to generate dedupe keys:

	"hash_fields": ["listings.name", "locations.address1", "lat_rounded", "lng_rounded"]

	‚Ä¢	Effect:
	‚Ä¢	Creates a unique string (hash) for detecting duplicates.
	‚Ä¢	Used in staging dedupe and optionally for logging.

‚∏ª

e) Dedupe Key Generation
	‚Ä¢	Combines normalized and hashed fields to form dedupe_key:
	
	"dedupe_key_fields": ["listings.name", "locations.address1", "lat_rounded", "lng_rounded"]

		‚Ä¢	Effect:
	‚Ä¢	One canonical key per logical entity.
	‚Ä¢	Helps identify duplicates within the import batch and later during promotion.

‚∏ª

3. Output of Transform Stage

After transform:
	‚Ä¢	All staging columns are filled, with standardized, mapped, and normalized values.
	‚Ä¢	dedupe_key and hash fields are ready for duplicate detection.
	‚Ä¢	Audit fields (imported_from, created_at, updated_at) can be added now.
	‚Ä¢	Transformed file is saved to S3 and ready for validation stage.

‚∏ª

4. Notes / Best Practices
	1.	Mapping comes first, normalization after mapping:
	‚Ä¢	You normalize the mapped DB fields, not raw CSV column names.
	2.	Do not round or hash before normalization:
	‚Ä¢	Otherwise, formatting inconsistencies might create different hash keys.
	3.	Staging table receives all fields, including transform-generated fields:
	‚Ä¢	Raw fields
	‚Ä¢	Normalized fields
	‚Ä¢	Rounded lat/lng
	‚Ä¢	Hash/dedupe keys

‚∏ª

üí° Summary:
Transform stage = raw import ‚Üí clean, normalized, mapped, dedupe-ready data.
	‚Ä¢	It prepares everything needed for validation and later staging without touching production yet.
	‚Ä¢	Outputs transformed file that serves as input to validation stage.

---

## Validation Stage

### Validation Stage Highlights

Purpose of the Validation Stage

The validation stage is the "data quality assurance" step:
	‚Ä¢	Validates transformed data against data types, business rules, and required fields.
	‚Ä¢	Computes data quality scores and identifies problematic records.
	‚Ä¢	Separates valid records from invalid ones for downstream processing.
	‚Ä¢	Outputs validated file containing only records that pass all validation checks.

1. **Data Type Validation** ‚Äì Ensure fields match expected types (numeric, email, phone, etc.).
2. **Required Field Checks** ‚Äì Verify all mandatory fields are present and non-empty.
3. **Business Rule Validation** ‚Äì Apply DMO-specific constraints and geographic bounds.
4. **Quality Scoring** ‚Äì Compute completeness, accuracy, and consistency scores.
5. **Error Reporting** ‚Äì Log validation failures with detailed error messages.
6. **Output** ‚Äì Validated file ready for staging load.

### Key Sub-Steps in Validation

a) Data Type Validation
	‚Ä¢	Validate field types against expected formats:
	‚Ä¢	Lat/lng must be numeric and within valid ranges
	‚Ä¢	Email addresses must match email format
	‚Ä¢	Phone numbers must match phone format
	‚Ä¢	Dates must be valid date formats
	‚Ä¢	Example:

"data_types": {
  "locations.lat": "numeric",
  "locations.lng": "numeric", 
  "contact_info.email": "email",
  "contact_info.local_phone": "phone"
}

	‚Ä¢	Effect:
	‚Ä¢	Invalid data types are flagged and excluded from validated output.
	‚Ä¢	Prevents downstream errors during staging and promotion.

‚∏ª

b) Required Field Checks
	‚Ä¢	Verify all mandatory fields are present:
	‚Ä¢	Example:

"required_fields": [
  "listings.name",
  "locations.address1",
  "locations.city"
]

	‚Ä¢	Effect:
	‚Ä¢	Records missing required fields are flagged for manual review.
	‚Ä¢	Ensures data completeness before staging.

‚∏ª

c) Business Rule Validation
	‚Ä¢	Apply DMO-specific constraints:
	‚Ä¢	Geographic bounds (lat/lng within DMO territory)
	‚Ä¢	Category restrictions
	‚Ä¢	Custom business logic
	‚Ä¢	Example:

"business_rules": {
  "lat_bounds": [-90, 90],
  "lng_bounds": [-180, 180],
  "dmo_geographic_bounds": {
    "min_lat": 40.0,
    "max_lat": 45.0,
    "min_lng": -80.0,
    "max_lng": -70.0
  }
}

	‚Ä¢	Effect:
	‚Ä¢	Records violating business rules are flagged for review.
	‚Ä¢	Maintains data integrity and DMO-specific requirements.

‚∏ª

d) Quality Scoring
	‚Ä¢	Compute data quality metrics:
	‚Ä¢	Completeness: percentage of fields populated
	‚Ä¢	Accuracy: validation of field formats and values
	‚Ä¢	Consistency: internal consistency checks
	‚Ä¢	Example:

"quality_scoring": {
  "enabled": true,
  "weights": {
    "completeness": 0.4,
    "accuracy": 0.3,
    "consistency": 0.3
  }
}

	‚Ä¢	Effect:
	‚Ä¢	Provides quantitative measure of data quality.
	‚Ä¢	Helps prioritize records for manual review.

‚∏ª

e) Error Reporting
	‚Ä¢	Log validation failures with detailed information:
	‚Ä¢	Record identifier
	‚Ä¢	Field that failed validation
	‚Ä¢	Validation rule that was violated
	‚Ä¢	Suggested correction
	‚Ä¢	Effect:
	‚Ä¢	Enables debugging and data correction.
	‚Ä¢	Stored in pipeline.job_errors for analysis.

‚∏ª

3. Output of Validation Stage

After validation:
	‚Ä¢	Validated file contains only records that pass all validation checks.
	‚Ä¢	Quality scores are computed for each record.
	‚Ä¢	Validation errors are logged in pipeline.job_errors.
	‚Ä¢	Validated file is ready for staging load.

‚∏ª

4. Notes / Best Practices
	1.	Fail fast for critical errors:
	‚Ä¢	Stop processing immediately for data type violations.
	‚Ä¢	Continue processing for warnings and optional issues.
	2.	Comprehensive error reporting:
	‚Ä¢	Include record context and suggested corrections.
	‚Ä¢	Enable efficient debugging and data correction.
	3.	Quality scoring:
	‚Ä¢	Use consistent scoring methodology across all DMOs.
	‚Ä¢	Provide actionable insights for data improvement.

‚∏ª

üí° Summary:
Validation stage = transformed data ‚Üí quality-assured, validated data.
	‚Ä¢	It ensures only high-quality data proceeds to staging and production.
	‚Ä¢	Provides comprehensive error reporting and quality metrics.



# how staging -> public promotion works

1. Staging contains everything
	‚Ä¢	Every record from your import (CSV, API, or other source) goes into the staging tables, even if:
		‚Ä¢	It‚Äôs a duplicate within the batch
		‚Ä¢	It might overwrite production data
		‚Ä¢	It‚Äôs missing some fields
	‚Ä¢	Staging is essentially a ‚Äúsandbox‚Äù:
		‚Ä¢	You can review, clean, normalize, and flag records before they affect production.
		‚Ä¢	All audit info (imported_from, created_at, updated_at) is stored here.

‚∏ª

2. Promotion selects only some rows
	‚Ä¢	Promotion is conditional, based on:
		‚Ä¢	ok_to_promote / status_field
		‚Ä¢	Deduplication against production (dedupe_key comparison)
		‚Ä¢	Merge/conflict rules (merge_policy)
		‚Ä¢	Manual override flags (manually_modified)
	‚Ä¢	Only rows that pass these checks are inserted or updated in production.

‚∏ª

3. Example flow
	1.	Import ‚Üí Staging
	‚Ä¢	All rows go in.
	‚Ä¢	Compute dedupe_key, normalize, round lat/lng, hash fields.
	‚Ä¢	Flag duplicates within staging, but keep all rows.
	2.	Staging review
	‚Ä¢	Optional human review or automated checks.
	‚Ä¢	Rows marked ok_to_promote = false stay in staging.
	3.	Promotion ‚Üí Production
	‚Ä¢	Compute dedupe key on production data.
	‚Ä¢	Compare staging rows to production: decide insert vs update vs skip.
	‚Ä¢	Apply merge_policy and respect manually_modified flags.
	‚Ä¢	Update bridge tables if needed (update_bridge_table = true).
	4.	Post-promotion
	‚Ä¢	Staging table keeps all rows for auditing.
	‚Ä¢	Production only contains clean, final, promoted rows.

‚∏ª

‚úÖ Rule of thumb:
	‚Ä¢	Staging = full import, with audit + dedupe info + review flags
	‚Ä¢	Production = clean, deduplicated, conflict-resolved data


‚∏ª

## S3 File Naming

### raw, transformed, and validated file names
s3://{environment: dev, staging, app}-trippl-data/{stage: raw, transformed, validated}/{data-domain: listings, reviews, events}/{country: xx}/{state/province: xx}/{dmo-id}/{date: YYYYMMDD}/{batch: nnn}-{timestamp: YYYYMMDDTHHMMSS}.{format: json, csv, xls}.gz

### image names
s3://{environment: dev, staging, app}-trippl-images/{country}/{state}/{dmo-id}/{listing-id}/{filename}.{ext}


## Admin pages
add permissions for new pipeline functions. add all permissions to the super_admin role.

all Pipeline menus and pages should be protected on the front and back end with required permission pipeline:read.

make a new sidebar menu section called Pipeline, visible to users with permission

make admin pages to manage all steps of the pipeline. 

admin pages should follow the structure of existing admin pages (topbar, sidebar etc)


## Job Scheduling
use manual scheduling/execution on localhost.

on dev/staging/app use aws SQS to schedule and execute jobs in the job_queue

**if dmo config changes** then add a dialog at save time that checks for scheduled jobs and lets user ignore or cancel.  Changes to schedule shouldn't be an issue, though an old job with old config might overwrite new job(s).


## suggested Pipeline Schema table design

```sql

example config_params:
"import": {
  "source_type": "csv",   -- 'csv', 'wordpress', 'drupal', 'website', 'api', 'eventbrite'
  "file_format": "csv",   -- 'csv', 'json'
  "delimiter": ",",
  "encoding": "utf-8",
  "skip_header": true
}

// do the field mapping in the transform step, not the load-to-staging step.
// That way, your staging table always looks like your internal canonical schema (name, address, lat, lng, etc.), regardless of source.

"transform": {
  "map_fields": {
    "listings.name": "Name",
    "locations.address1": "Street Address",
    "contact_info.email": "Email"
  },
  "normalize_fields": [
    "listings.name",
    "locations.address1",
    "listings.category"
  ],
  "round_lat_lng": 5,
  "hash_fields": ["listings.name", "locations.address1", "lat", "lng", "listings.category"],
  "dedupe_key_fields": ["listings.name", "locations.address1", "lat", "lng"]
}

"validation": {
  "data_types": {
    "locations.lat": "numeric",
    "locations.lng": "numeric",
    "contact_info.email": "email",
    "contact_info.local_phone": "phone"
  },
  "required_fields": [
    "listings.name",
    "locations.address1",
    "locations.city"
  ],
  "business_rules": {
    "lat_bounds": [-90, 90],
    "lng_bounds": [-180, 180],
    "dmo_geographic_bounds": {
      "min_lat": 40.0,
      "max_lat": 45.0,
      "min_lng": -80.0,
      "max_lng": -70.0
    }
  },
  "quality_scoring": {
    "enabled": true,
    "weights": {
      "completeness": 0.4,
      "accuracy": 0.3,
      "consistency": 0.3
    }
  }
}

"staging": {
  "match_on": [
    "listings.dedupe_key",
    "locations.dedupe_key",
    "contact_info.dedupe_key"
  ],
  "conflict_resolution": "update_if_changed",
  "status_field": {
    "listings": "ok_to_promote",
    "locations": "ok_to_promote",
    "contact_info": "ok_to_promote"
  },
  "audit_fields": ["created_at", "updated_at", "imported_from"]
}

"promotion": {
  "merge_policy": "latest_wins", // if a row exists then staging version overwrites it
  // "merge_policy": "skip_if_exists" // keep production data if it already exists
  // "merge_policy": "field_level_merge" // update only fields that changed
  "update_bridge_table": true  // using a bridge table for many-to-many relationships, set to true to ensure relationships are updated during promotion. If false, only the main tables (listings, locations) are updated, but bridge/assocaiation tables are left stale (?)
  "audit_fields": ["promoted_at", "promoted_by", "imported_from"],
  "conflict_resolution": {
    "listings": "latest_wins",
    "locations": "field_level_merge",
    "contact_info": "latest_wins"
  }
}

CREATE TABLE pipeline.dmo_configs (
id              SERIAL PRIMARY KEY,                       ‚Äì unique config row
dmo_id          INT NOT NULL REFERENCES public.dmos(id),  ‚Äì which DMO this config belongs to
type            VARCHAR(20) NOT NULL,                     ‚Äì stage type: 'import', 'transform', 'validate', 'stage', 'promote'
data_domain     TEXT NOT NULL,                            ‚Äì e.g., 'listings', 'events', 'reviews'
config_params   JSONB NOT NULL,                           ‚Äì JSON with rules relevant to this stage
version         INT NOT NULL DEFAULT 1,                   ‚Äì increment if changing this config
schedule        TEXT NOT NULL,                             ‚Äì e.g., 'manual', 'pipeline', 'hourly', 'daily', 'weekly', 'monthly'
active          BOOLEAN DEFAULT TRUE,                     ‚Äì is this config currently active?
created_at      TIMESTAMP DEFAULT now(),                ‚Äì timestamp when created
updated_at      TIMESTAMP DEFAULT now()                 ‚Äì timestamp when last updated
);

ALTER TABLE pipeline.dmo_configs
ADD CONSTRAINT type_enum CHECK (type IN ('import', 'transform', 'validate', 'stage', 'promote'));
‚Äì ensures type is valid

‚∏ª

CREATE TABLE pipeline.job_queue (
job_id          SERIAL PRIMARY KEY,                       ‚Äì unique job identifier
job_type        VARCHAR(50) NOT NULL,                     ‚Äì 'import_raw', 'transform', 'validate', 'load_stage', 'promote_public'
status          VARCHAR(20) NOT NULL DEFAULT 'pending',   ‚Äì job status: pending, running, completed, failed, cancelled
priority        INT DEFAULT 0,                             ‚Äì optional job priority
source_name     TEXT,                                      ‚Äì optional name of data source
enqueued_at     TIMESTAMP DEFAULT now(),                 ‚Äì timestamp when job was added to queue
started_at      TIMESTAMP,                               ‚Äì timestamp when job started
finished_at     TIMESTAMP,                               ‚Äì timestamp when job finished
dequeue_attempts INT DEFAULT 0,                             ‚Äì number of times job was dequeued
next_attempt_at TIMESTAMP,                               ‚Äì when to try next if failed
locked_by       TEXT,                                      ‚Äì worker that locked the job
locked_at       TIMESTAMP,                               ‚Äì timestamp when job was locked
retry_count     INT DEFAULT 0,                              ‚Äì total retries attempted
error_message   TEXT,                                      ‚Äì summary error message if job failed
config_snapshot_id UUID REFERENCES pipeline.job_configs(snapshot_id) ON DELETE SET NULL
);

CREATE INDEX idx_job_queue_pending ON pipeline.job_queue(status, next_attempt_at);
‚Äì helps workers quickly find pending jobs that are ready to run

‚∏ª

CREATE TABLE pipeline.job_configs (
snapshot_id     SERIAL PRIMARY KEY,						 ‚Äì unique snapshot identifier
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
dmo_id          INT NOT NULL REFERENCES public.dmos(id),   ‚Äì which DMO this snapshot belongs to
type            VARCHAR(20) NOT NULL,                      ‚Äì stage type: 'import', 'transform', 'validate', 'stage', 'promote'
data_domain     TEXT NOT NULL,                              ‚Äì e.g., 'listings', 'events', 'reviews'
config_params   JSONB NOT NULL,                             ‚Äì copied config for reproducibility
version         INT NOT NULL DEFAULT 1,                     ‚Äì snapshot version
schedule        TEXT NOT NULL,                               ‚Äì schedule info from original config
created_at      TIMESTAMP DEFAULT now(),                  ‚Äì when snapshot was created
updated_at      TIMESTAMP DEFAULT now()                   ‚Äì last updated timestamp
);

‚∏ª

CREATE TABLE pipeline.files (
id              SERIAL PRIMARY KEY,                         ‚Äì unique file record
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
file_path       TEXT NOT NULL,                               ‚Äì path to file
stage           VARCHAR(20) NOT NULL,                        ‚Äì stage: 'import', 'transform', 'validate', 'stage', 'promote'
data_domain     TEXT,                                        ‚Äì optional: 'listings', 'events', 'reviews'
country         TEXT,                                        ‚Äì optional country info
dmo_id          INT,                                         ‚Äì optional DMO ID
file_date       DATE,                                        ‚Äì optional file date
batch           INT,                                         ‚Äì optional batch number
format          VARCHAR(10),                                 ‚Äì file format: 'csv', 'json', 'xls'
file_size       BIGINT,                                      ‚Äì size in bytes
record_count    INT,                                         ‚Äì number of records in file
file_hash       TEXT,                                        ‚Äì optional hash for deduplication
created_at      TIMESTAMP DEFAULT now()                    ‚Äì timestamp file was created
);

CREATE INDEX idx_files_job_stage ON pipeline.files(job_id, stage);
‚Äì fast lookup for files per job and stage

‚∏ª

CREATE TABLE pipeline.job_results (
job_id          INT PRIMARY KEY REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
records_total   INT,                                         ‚Äì total records processed
records_new     INT,                                         ‚Äì new records inserted
records_updated INT,                                         ‚Äì updated records
records_deleted INT,                                         ‚Äì deleted records
records_duplicates INT,                                      ‚Äì duplicates detected
records_unchanged INT,                                       ‚Äì unchanged records
records_error   INT,                                         ‚Äì records with errors
created_at      TIMESTAMP DEFAULT now()
);

‚∏ª

CREATE TABLE pipeline.job_errors (
id              BIGSERIAL PRIMARY KEY,
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
file_id         INT REFERENCES pipeline.files(id),           ‚Äì file where error occurred
record_id       TEXT,                                        ‚Äì unique record identifier / hash
original_record JSONB,                                       ‚Äì full original record
error_type      TEXT,                                        ‚Äì type of error, e.g., parse_error
error_message   TEXT,                                        ‚Äì descriptive error message
created_at      TIMESTAMP DEFAULT now()
);

‚∏ª

CREATE TABLE pipeline.bridge_source_listing (
id                  SERIAL PRIMARY KEY,
job_file_id         INT NOT NULL REFERENCES pipeline.files(id),
source_record_hash  TEXT NOT NULL,                           ‚Äì hash of source record for mapping
listing_id          INT REFERENCES public.listings(id),       ‚Äì canonical listing ID
certainty_score     NUMERIC(5,4) DEFAULT 1.0,                ‚Äì confidence 0.0‚Äì1.0
mapped_at           TIMESTAMP DEFAULT now(),               ‚Äì when mapping occurred
error_type          TEXT,                                     ‚Äì optional mapping error type
error_message       TEXT,                                     ‚Äì optional error message
UNIQUE (job_file_id, source_record_hash)
);

CREATE INDEX idx_bridge_listing ON pipeline.bridge_source_listing(listing_id);
‚Äì fast lookup for canonical listings

‚∏ª

CREATE TABLE pipeline.bridge_source_listing_info (
id SERIAL PRIMARY KEY,
job_file_id INT NOT NULL REFERENCES pipeline.files(id),
source_record_hash TEXT NOT NULL, ‚Äì hash of source record for mapping
listing_info_id INT REFERENCES public.listing_info(id), ‚Äì canonical listing_info ID
certainty_score NUMERIC(5,4) DEFAULT 1.0, ‚Äì confidence 0.0‚Äì1.0
mapped_at TIMESTAMP DEFAULT now(), ‚Äì when mapping occurred
error_type TEXT, ‚Äì optional mapping error type
error_message TEXT, ‚Äì optional error message
UNIQUE (job_file_id, source_record_hash)
);

CREATE INDEX idx_bridge_listing_info ON pipeline.bridge_source_listing_info(listing_info_id);
‚Äì fast lookup for canonical listing_info

```

## Suggesting Staging Schema

**Schema name:** staging
-	Stores all imported/transformed rows before promotion.
-	Keeps full audit trail, dedupe keys, hashes, and flags.
-	All raw fields are preserved, along with normalized/hashed versions for deduplication.

```sql

-- Listings table (staging)
CREATE TABLE staging.listings (
    id serial PRIMARY KEY,
    dmo_id integer NULL,
    name character varying NULL,
    google_place_id character varying NULL,
    location_id integer NULL,
    listing_visible boolean DEFAULT true,
    listing_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    -- dedupe/audit fields
    dedupe_key character varying NOT NULL,
    hash_name character varying,
    hash_address character varying,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now(),
    ok_to_promote boolean DEFAULT false
);

-- Listing Info (staging)
CREATE TABLE staging.listing_info (
    id serial PRIMARY KEY,
    listing_id integer NULL,
    description character varying NULL,
    contact_info_id integer NULL,
    info_visible boolean DEFAULT true,
    info_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now(),
    ok_to_promote boolean DEFAULT false
);

-- Contact Info (staging)
CREATE TABLE staging.contact_info (
    id serial PRIMARY KEY,
    name character varying NULL,
    email character varying NULL,
    local_phone character varying NULL,
    international_phone character varying NULL,
    website_url character varying NULL,
    facebook_url character varying NULL,
    instagram_url character varying NULL,
    youtube_url character varying NULL,
    tiktok_url character varying NULL,
    twitter_url character varying NULL,
    pinterest_url character varying NULL,
    tripadvisor_url character varying NULL,
    yelp_url character varying NULL,
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Locations (staging)
CREATE TABLE staging.locations (
    id serial PRIMARY KEY,
    address1 character varying NULL,
    address2 character varying NULL,
    city character varying NULL,
    state character varying NULL,
    zip character varying NULL,
    country char(2) NULL,
    lat numeric(10,8) NULL,
    lng numeric(11,8) NULL,
    lat_rounded numeric(10,5),
    lng_rounded numeric(11,5),
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Listing Images (staging)
CREATE TABLE staging.listing_images (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    image character varying NULL,
    rank integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Dates (staging)
CREATE TABLE staging.opening_dates (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    start_date character varying NULL,
    end_date character varying NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Hours (staging)
CREATE TABLE staging.opening_hours (
    id serial PRIMARY KEY,
    opening_date_id integer NULL,
    day_id integer NULL,
    open_time character varying NULL,
    close_time character varying NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Bridge tables (staging)
CREATE TABLE staging.bridge_attributes_listings (
    id serial PRIMARY KEY,
    attribute_id integer NULL,
    listing_id integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);
```

**Notes on staging:**
-	dedupe_key is always computed using a hash of normalized key fields (e.g., name + address + lat/lng).
-	ok_to_promote is false by default, set to true only after review.
-	All imported fields are kept to allow audit and traceability.

## Suggesting Public Schema (updates to existing schema)

**Schema name:** public (existing schema)
-	Only clean, reviewed, and promoted rows go here.
-	Deduplication is computed on-the-fly during promotion; dedupe_key is optional to store (can be omitted to keep schema clean).
-	Audit fields track promotion and manual edits.

```sql
-- Listings table (production)
CREATE TABLE public.listings (
    id serial PRIMARY KEY,
    dmo_id integer NULL,
    name character varying NULL,
    google_place_id character varying NULL,
    location_id integer NULL,
    listing_visible boolean DEFAULT true,
    listing_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Listing Info (production)
CREATE TABLE public.listing_info (
    id serial PRIMARY KEY,
    listing_id integer NULL,
    description character varying NULL,
    contact_info_id integer NULL,
    info_visible boolean DEFAULT true,
    info_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Contact Info (production)
CREATE TABLE public.contact_info (
    id serial PRIMARY KEY,
    name character varying NULL,
    email character varying NULL,
    local_phone character varying NULL,
    international_phone character varying NULL,
    website_url character varying NULL,
    facebook_url character varying NULL,
    instagram_url character varying NULL,
    youtube_url character varying NULL,
    tiktok_url character varying NULL,
    twitter_url character varying NULL,
    pinterest_url character varying NULL,
    tripadvisor_url character varying NULL,
    yelp_url character varying NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Locations (production)
CREATE TABLE public.locations (
    id serial PRIMARY KEY,
    address1 character varying NULL,
    address2 character varying NULL,
    city character varying NULL,
    state character varying NULL,
    zip character varying NULL,
    country char(2) NULL,
    lat numeric(10,8) NULL,
    lng numeric(11,8) NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Listing Images (production)
CREATE TABLE public.listing_images (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    image character varying NULL,
    rank integer NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Dates (production)
CREATE TABLE public.opening_dates (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    start_date character varying NULL,
    end_date character varying NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Hours (production)
CREATE TABLE public.opening_hours (
    id serial PRIMARY KEY,
    opening_date_id integer NULL,
    day_id integer NULL,
    open_time character varying NULL,
    close_time character varying NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Bridge tables (production)
CREATE TABLE public.bridge_attributes_listings (
    id serial PRIMARY KEY,
    attribute_id integer NULL,
    listing_id integer NULL,
    promoted_at timestamp NULL,
    promoted_by integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);
```
**Notes on production public schema:**
-	Only reviewed and promoted rows live here.
-	Deduplication is applied during promotion; no dedupe keys stored unless needed for performance.
-	promoted_at and promoted_by track the exact promotion event.
-	manually_modified flags ensure pipeline doesn‚Äôt overwrite manual edits.

‚úÖ Key Design Points for Enterprise-Grade Traceability
1.	Staging = full audit, full imported data, dedupe & normalized fields
2.	Production = clean, promoted data, audit of promotion + manual edits
3.	Multi-table mapping is explicit: listings.name, contact_info.email, locations.address1
4.	Deduplication & hashing: staging generates dedupe_key per record; production dedupe is computed on-the-fly during promotion
5.	Audit fields everywhere: imported_from, created_at, updated_at, promoted_at, promoted_by
6.	Bridge tables follow the same pattern for relational mapping, with promotion/audit fields


# Step by step parent/sub-table deduplication, hashing, promotion, and matching logic. 

# Listings Pipeline: Parent and Sub-Table Promotion

## 1. Parent Tables

| Table | Key Fields / Dedupe Key | Promotion Flag |
|-------|------------------------|----------------|
| `listings` | `name + address + lat/lng` | `ok_to_promote` |
| `listing_info` | `description + contact_info_id` | `ok_to_promote` |

- `dedupe_key` is computed in staging from key identifying fields.  
- Only rows with `ok_to_promote = true` are eligible for promotion.  
- Matching during promotion is done against production tables using `dedupe_key`.

---

## 2. Sub-Tables

Promotion of sub-tables is **conditional on parent approval**. Each sub-table has its own dedupe key used for matching against production.

| Sub-Table | Key Fields for Hash / Dedupe | Linked To | Notes on Matching / Promotion |
|-----------|-----------------------------|-----------|-------------------------------|
| `contact_info` | `email + phone + name` | `listing_info.contact_info_id` | Reuse existing row in production if dedupe key matches; else insert new. |
| `locations` | `address1 + city + state + zip + country` | `listings.location_id` | Same: match dedupe key, reuse or insert new. |
| `listing_images` | `listing_info_id + image_url` | `listing_info.id` | Avoid duplicate images per listing. |
| `opening_dates` | `listing_info_id + start_date + end_date` | `listing_info.id` | Prevent duplicate date ranges for same listing_info. |
| `opening_hours` | `opening_date_id + day_id + open_time + close_time` | `opening_dates.id` | Match exact schedule; parent opening_date must exist. |
| `bridge_attributes_listings` | `listing_id + attribute_id` | `listings.id` | Deduplicate the pair of foreign keys; parent listing must be promoted. |

---

## 3. Promotion Flow

1. Compute **dedupe keys** for all parent and sub-table staging rows.  
2. Review parent rows and set `ok_to_promote`.  
3. For each parent row approved:
   - Promote parent row to production.  
   - For each linked sub-table:
     - Check if `dedupe_key` exists in production.
     - **If exists:** reuse production ID.  
     - **If not exists:** insert new row with audit fields.  
   - Update parent table foreign keys to point to promoted sub-table IDs.  
4. Maintain audit fields throughout:
   - `imported_from`  
   - `created_at`, `updated_at`  
   - `promoted_at`, `promoted_by`  

---

## 4. Key Principles

- Sub-tables **do not promote independently**; promotion is controlled by parent `ok_to_promote`.  
- Deduplication ensures **no duplicate contact_info, locations, or images** across multiple listings.  
- Hash/dedupe keys in staging provide **reliable matching** during promotion.  
- Bridge tables follow the same dedupe/matching rules as sub-tables.  
- All audit fields are preserved for **enterprise-grade traceability**.

## dedupe_key in staging
Exists only in staging tables (you don‚Äôt store it in production).
	‚Ä¢	It‚Äôs generated on-the-fly during the transform step, usually by hashing a combination of normalized fields, e.g.:

hash(name_normalized + address_normalized + lat_rounded + lng_rounded)	

	‚Ä¢	Purpose: to detect duplicates across imports and multiple sources before promotion.

‚∏ª

2. Comparing to production
	‚Ä¢	During the staging ‚Üí production promotion step, the staging dedupe_key is compared to an on-the-fly dedupe key generated for production rows.
	‚Ä¢	Production rows don‚Äôt store dedupe_key, but you recompute it using the same hashing logic on production data fields (name, address, lat/lng, etc.).
	‚Ä¢	If a match is found:
	‚Ä¢	Conflict resolution is triggered (e.g., latest_wins, field_level_merge, skip_if_exists).
	‚Ä¢	If no match is found:
	‚Ä¢	Staging row is inserted as a new row in production.

‚∏ª

3. Why not store dedupe_key in production?
	‚Ä¢	Keeps production schema clean ‚Äî no temporary hash fields.
	‚Ä¢	Avoids stale dedupe keys if production data is edited manually (manually_modified).
	‚Ä¢	Staging handles all deduplication logic, so production only sees final clean rows.

‚∏ª

4. Optional enhancements
	‚Ä¢	Some pipelines do store dedupe_key in production for speed, but then:
	‚Ä¢	You need to recompute it anytime a row is manually modified, which adds complexity.
	‚Ä¢	Usually easier and safer to compute it on-the-fly during promotion.

‚∏ª

üí° Rule of thumb:
	‚Ä¢	Staging = dedupe + conflict detection + audit fields
	‚Ä¢	Production = clean, normalized data + audit of creation/promotion + manual edit flags

