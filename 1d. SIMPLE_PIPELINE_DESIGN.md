# Listings Pipeline Design
**goal:** implement an enterprise grade listings pipeline with a powerful graphical interface for managing data pipeline.

## Pipeline Stages

1. **Configure** ‚Äì Set pipeline rules for a DMO.
2. **Import** ‚Äì Upload CSV or import .json, store as Raw File.
3. **Transform** ‚Äì Map, normalize, hash, dedupe, and assign **internal canonical IDs** before staging.
4. **Validate** ‚Äì Data type validation, required field checks, business rules, and data quality scoring.
5. **Stage** ‚Äì Load validated data to staging schema for review.
6. **Review** ‚Äì Manual verification; resolve low certainty or conflicts, set `ok_to_promote`.
7. **Promote** ‚Äì Move approved rows to production; sub-tables promote conditionally based on parent.

---

## Key Concepts


### Source_id
- per-record unique identifier from the original source system (WordPress slug, Priceline ID, Foursquare ID, etc.)
- Can detect duplicates and support upserts
- Is deterministic (same record ‚Üí same ID)
- Comes with the imported record; exists before Transform. It is used to check if this record already exists in your DB.
- an input record has one source_id, which can be used to lookup listings or listing_info
- In practice:
  - use source_id to lookup listing in DB.
    - not exist - stage new listing+listing_info.
    - exist
      - lookup listing_info using source_id
        - not exist - stage new listing_info
        - exist
          - check hashes
            - changes - stage for updates
            - no changes - stage for skipped

- one external record maps to a listing and a listing_info
- the same source_id is used for the listing and the listing_info
- for each external record, there will be a one possible listing_info match
- for each listing, there could be multiple matches to import records, from different dmos

## what if the import type changes for the same source (dmo_id)
**e.g. BookingPEI**
source: csv      record: 1   hash: dmo_id+name+city  = 51

source: wp api   record: 1   hash: dmo_id+name+city  = 51

- so the determining factor for the source_id will be the dmo and listing details.
- it won't matter if we change the input type (csv, wordpress), the source_id should stay the same.
- that way we will be doing an update of an existing record, if we did a different import type.

## how to generate source_id
| source        | source_id                                   |
|---------------|---------------------------------------------|
| Priceline     | id                                          |
| Foursquare    | id                                          |
| Google        | placeId                                     |
| csv           | dmo_id + normalize(name) + normalize(city)  |
| wordpress api | (dont need dmo_id?) source_url? or same hash as csv? |
| BPEI website  | 
| CC website    |
| PE website    |
| TPEI website  | 

```
// fields for normalizing: dmo_id, name, address, city, postal_code, source_url
const normalize = str =>
  str?.toLowerCase().trim().replace(/[^\w\s]/g, '') || '';

import crypto from 'crypto';
const sourceIdentifier = crypto
  .createHash('sha1')
  .update(`${source_id}|${normalize(name)}|${normalize(address)}|${normalize(city)}|${normalize(country)}`)
  .digest('hex');
```

## field type
Need one field (source_id) that can store any kind of unique identifier, regardless of source ‚Äî e.g.:
- ChIJN1t_tDeuEmsRUsoyG83frY4 (Google Place ID)
- H1234567890 (Priceline hotel ID)
- https://example.com/listing/123 (WordPress permalink) - or we use the same as csv, in case of changing import type, the records will still evaluate to the same source
- abc123xyz... (hash generated from CSV fields)

```sql
source_id TEXT NOT NULL
-- Can handle strings of any length (no need to specify a max).
-- PostgreSQL stores short strings efficiently (inline up to ~2kB).
-- Works for IDs, URLs, and hashes alike.
-- Lets you normalize across sources without worrying about type casting.

// unique across a single source (dmo_id) -- ie google, priceline, bookingpei.com. question: then what about reviews, same dmo_id?
CONSTRAINT unique_source_id_per_source UNIQUE (dmo_id, source_id)
```
## mapping the source_id to listings and listing_info
- there is one source_id for an import record. so listing and listing_info will have the same source_id.
- Use one shared bridge table (source_mappings) with a target_type column

```sql
CREATE TABLE pipeline.bridge_source_target (
  id SERIAL PRIMARY KEY,
  dmo_id INT NOT NULL,              -- distinguishes different sources (ie, google and priceline could have a listing with the same id)
  source_id TEXT NOT NULL,          -- e.g. 'ChIJN1t_tDeuEmsRUsoyG83frY4'
  target_type TEXT NOT NULL,        -- e.g. 'listing', 'listing_info'
  target_id INT NOT NULL,           -- internal DB record ID
  UNIQUE (dmo_id, source_id, target_type)
);

Example Data:

dmo_id      source_id                     target_type     target_id
200 (PL)    ChIJN1t_tDeuEmsRUsoyG83frY4   listing         101
200 (PL)    ChIJN1t_tDeuEmsRUsoyG83frY4   listing_info    201
1 (google)  ChIJN1t_tDeuEmsRUsoyG83frY4   listing         60
5 (bpei)    xxx                           listing         101
5 (bpei)    xxx                           listing_info    500

-- FIND THE DB RECORD
SELECT target_type, target_id
FROM pipeline.bridge_source_target
WHERE dmo_id = 200
  AND source_id = 'ChIJN1t_tDeuEmsRUsoyG83frY4';
```

                      +---------------------------+
                      | bridge_source_target      |
                      |---------------------------|
                      | id (PK)                   |  <-- primary key index
                      | dmo_id                    |
                      | source_id                 |
                      | target_id                 |
                      | target_type               |
                      | certainty_score           |
                      | mapped_at                 |
                      +---------------------------+
                               ^
                               |
                               |
          ----------------------------------------------
          |                                            |
          |                                            |
   1) Deduplication check                         2) Lookup by target_id
   (Check if source record already exists)        (Find internal record mappings)
          |                                            |
  SELECT * FROM bridge_source_target           SELECT source_id, dmo_id
  WHERE dmo_id = ?                              FROM bridge_source_target
    AND source_id = ?                             WHERE target_id = ?
    AND target_type = ?                                AND target_type = ?


### Deterministic Internal IDs
- **Internal IDs are SERIAL IDs**: We use the database SERIAL primary keys (`public.listings.id`, `public.listing_info.id`, etc.) as our deterministic canonical identifiers.
- **Generation occurs during Promotion stage**: When data is promoted from staging to production, PostgreSQL automatically assigns SERIAL IDs which become our canonical internal IDs.
- Maps multiple source IDs (DMO, website, Priceline, GooglePlaceID) to a single internal `listing_id`.
- Maps one-to-one source IDs to a single internal `listing_info_id`.
- Mappings stored in `pipeline.bridge_source_target`.
- Ensures **consistent internal IDs** across multiple sources and imports.
- **Conflict Resolution Rules:**
  - **Listings level**: Multiple sources can map to the same `listing_id`
  - **Listing info level**: Only one source should map to a given `listing_info_id`
  - **Multiple listing_info per listing**: Allowed and expected, as long as IDs are unique.

### Internal ID Workflow
1. **Import Stage**: record/generate internal_id. Priceline or Google listing, internal_id is the place_id or priceline_id. A
2. **Transform Stage**: Look up existing internal IDs via `source_id` (bridge table) or `match_hash` (public table)
3. **Staging**: Store `listing_internal_id` to reference existing serial ID or leave null for new records
4. **Promotion Stage**: 
   - **New records**: INSERT to production ‚Üí PostgreSQL assigns SERIAL ID ‚Üí Update staging table ‚Üí Populate bridge table
   - **Existing records**: UPDATE existing production record using known SERIAL ID from staging
5. **Bridge Table**: Always contain the mapping `source_id` ‚Üí `target_id` (the SERIAL ID from public.listings.id or public.listings_info.id)

### Deduplication
- **Staging only:** `dedupe_key` generated per table via hash of normalized fields (e.g., `name + address + lat/lng`).
- **Production:** dedupe_key recomputed on-the-fly for matching; not stored in production.
- Sub-tables (`contact_info`, `locations`, `listing_images`, `opening_dates`, `opening_hours`, bridge tables) use parent-driven promotion and their own dedupe keys.

---

## Pipeline Steps (Implementation Summary)

| Step | Purpose / Action | Key Fields / Flags | Notes / Implementation Details |
|------|-----------------|-----------------|--------------------------------|
| **1. Configure** | Define pipeline behavior for a specific DMO | `field_mappings`, `normalize_rules`, `dedupe_fields`, `merge_policy`, `update_bridge_table` | Store pipeline settings per DMO; drives all downstream steps including transform, staging, and promotion. Determines how CSV/API fields map to internal tables. |
| **2. Import** | Load raw data | `raw_file_path`, `imported_from`, `imported_at`, `uploaded_by`, `dmo_id`, `source_location`, `recorded_at` | Upload CSV, JSON, or API data. Save original files to preserve audit trail. Minimal validation occurs at this stage. |
| **3. Transform** | Clean, normalize, map, dedupe, and assign internal IDs | `map_fields`, `normalize_fields`, `round_lat_lng`, `hash_fields`, `dedupe_key`, `listing_internal_id` | Apply CSV/API ‚Üí DB column mapping. Normalize strings, round lat/lng, compute hashes for deduplication. Look up existing internal IDs via source_id in bridge tables or match_hash in public tables. Set `listing_internal_id` in staging (SERIAL ID from public.listings.id). Output transformed file ready for validation. |
| **4. Validate** | Data type validation, required field checks, business rules | `validation_rules`, `data_types`, `required_fields`, `business_rules`, `quality_score` | Validate data types, check required fields, apply business rules (e.g., lat/lng within bounds), compute data quality scores. Fail fast for critical errors, collect warnings for optional issues. Output validated file ready for staging. |
| **5. Stage** | Load validated data into staging schema | `staging_table_rows`, `dedupe_key`, `imported_from`, `created_at`, `updated_at`, `ok_to_promote`, `audit_fields` | Insert all validated rows from validated file. Detect duplicates within the batch. Set `ok_to_promote` flags based on validation results. Keep full audit trail. Sub-tables (contact_info, locations, images, opening_dates, opening_hours, bridge tables) get staged and linked to parent IDs. |
| **6. Review** | Manual verification before promotion | `ok_to_promote`, `conflict_flags`, `add/update/delete indicators` | Show staging rows grouped by dedupe_key. Highlight changes: additions, updates, deletions. Allow manual approval. Resolve intra-stage conflicts. Only rows marked `ok_to_promote = true` are eligible for promotion. |
| **7. Promote** | Move approved rows to production | `merge_policy`, `update_bridge_table`, `promoted_at`, `promoted_by` | Compare dedupe_key with production rows on-the-fly. Insert new rows (PostgreSQL assigns SERIAL IDs) or update existing rows according to merge_policy. Update staging table with actual SERIAL IDs after insertion. Update bridge tables (`source_id` ‚Üí SERIAL ID mapping). Record audit: promoted_at, promoted_by, imported_from. |

### Pipeline Stages additional information

1. Configure
	‚Ä¢	Purpose: Define pipeline behavior for a specific DMO (data source).
	‚Ä¢	Key Details:
	‚Ä¢	Source info (name, file type, API credentials).
	‚Ä¢	Field mapping rules (CSV column ‚Üí DB column).
	‚Ä¢	Normalization rules (uppercase, trim, clean special chars).
	‚Ä¢	Deduplication settings (fields to hash, rounding precision for lat/lng).
	‚Ä¢	Promotion policies (merge_policy, update_bridge_table).

‚∏ª

2. Import
	‚Ä¢	Purpose: Ingest raw data.
	‚Ä¢	Key Details:
	‚Ä¢	Upload CSV, JSON, or API data.
	‚Ä¢	Save original as Raw File in staging area (audit trail).
	‚Ä¢	Store metadata: import timestamp, user, source identifier.
	‚Ä¢	Optionally validate basic CSV structure before processing.

‚∏ª

3. Transform
	‚Ä¢	Purpose: Clean, normalize, and prepare data for validation.
	‚Ä¢	Key Details:
	‚Ä¢	Map source columns ‚Üí DB fields.
	‚Ä¢	Normalize fields (trim, lowercase, remove special chars).
	‚Ä¢   Check source_id in public DB ‚Üí
   		‚îú‚îÄ If found ‚Üí copy lat/lng & address
   		‚îî‚îÄ If not found ‚Üí geocode ‚Üí assign lat/lng & address ‚Üí assign new internal ID
	‚Ä¢	Round lat/lng if used in dedupe.
	‚Ä¢	Compute hash fields for dedupe.
	‚Ä¢	Generate staging dedupe_key.
	‚Ä¢	Produce Transformed File ready for validation.

‚∏ª

4. Validate
	‚Ä¢	Purpose: Validate transformed data and prepare for staging.
	‚Ä¢	Key Details:
	‚Ä¢	Data type validation (lat/lng bounds, email format, phone format, etc.).
	‚Ä¢	Required field checks based on DMO configuration.
	‚Ä¢	Business rule validation (DMO-specific constraints, geographic bounds).
	‚Ä¢	Data quality scoring (optional for Phase 2).
	‚Ä¢	Fail fast for critical errors, collect warnings for optional issues.
	‚Ä¢	Output Validated File ready for staging load.
	‚Ä¢	Validation results stored in pipeline.job_errors for failed records.

‚∏ª

5. Stage
	‚Ä¢	Purpose: Load validated data into staging schema for review.
	‚Ä¢	Key Details:
	‚Ä¢	Insert all validated records from validated file into staging tables.
	‚Ä¢	Compute staging-only fields (dedupe_key, imported_from, audit fields).
	‚Ä¢	Detect duplicates within staging and optionally mark conflicts.
	‚Ä¢	Set initial promotion flags/status (ok_to_promote = false if duplicate/missing info).
	‚Ä¢	Keep all rows for full audit/review purposes.

‚∏ª

6. Review
	‚Ä¢	Purpose: Manual verification before promotion.
	‚Ä¢	Key Details:
	‚Ä¢	Display staging rows grouped by dedupe_key.
	‚Ä¢	Highlight: additions, updates, potential deletions.
	‚Ä¢	Allow user to mark rows ok_to_promote or leave blocked.
	‚Ä¢	Resolve intra-stage conflicts manually if needed.
	‚Ä¢	Once verified, click "Promote" to trigger promotion stage.

‚∏ª

7. Promote
	‚Ä¢	Purpose: Move verified staging data to production (public schema).
	‚Ä¢	Key Details:
	‚Ä¢	Compare dedupe_key with production rows on-the-fly.
	‚Ä¢	Insert new rows or update existing rows according to merge_policy.
	‚Ä¢	Respect manually_modified flags to avoid overwriting manual edits.
	‚Ä¢	Update bridge tables (e.g., attributes relationships) if needed.
	‚Ä¢	Record audit: promoted_at, promoted_by, imported_from.
	‚Ä¢	Optional cleanup or archiving of staging batch.


### Additional things for Cursor to implement directly
- Implement real geocoding API calls in Transform stage if lat/lng missing
- Implement validation stage with data type checks, required field validation, and business rules
- Save validated data to validated file for staging consumption
- Ensure staging.status field is respected during manual review
- Idempotency: repeated promotions should not create duplicates
- Job queue workers: handle retry_count, locked_by, dequeue_attempts, next_attempt_at
- Admin UI: simple staging review table with promote button
- Only a minimal subset of listing fields needs to be handled for MVP (name, address, lat, lng)

‚∏ª

# Stages details

## Import stage
**Metadata added to raw import files**
when a file is uploaded, we need to add extra columns with extra information before saving the raw import file with the S3 file name as indicated in the SIMPLE_PIPELINE_DESIGN file

the columns that we need added to the raw import file are:

- rec_#: sequential id of file record (each file in a batch starts at 0)
- batch_id: batch number. also in file name.
- dmo_id: dmo_id from the import config
- source_id: id (google PlaceId, Priceline property id etc) or in the case of a csv - a generated source_id (match_hash) = sha_256(dmo_id, normalize(listing_name), normalize(loc_city))
- imported_at: timestamp
- imported_by: account id of importer (0 for pipeline system)



### Sample import file csv file headers (or .json file fields)
SourceId,Name,AlternateNames,NameCode,Description,Category,SubCategories,
Region,Address1,Address2,City,State,Zip,Country,Lat,Lng,Timezone,UtcOffset,Elevation,
ContactName,Email,LocalPhone,InternationalPhone,
Website,Facebook,Instagram,Youtube,Tiktok,Twitter,Pinterest,TripAdvisor,Yelp,
Images,
Languages,Rating,Reviews,Price,Amenities,Seasons,Hours,OpenedDate,ClosedDate,Other

**Notes**
-	Each row corresponds to one listing.
-	Multi-value fields (images, opening dates, attributes) are repeated horizontally (Image1, Image2, etc.).
-	Empty cells are allowed; the transform step can ignore or fill defaults.
-	No table prefixes are needed ‚Äî the transform configuration handles mapping CSV columns to tables/fields.


### Sample transform field mappings

"transform": {
  "map_fields": {
    "listings.name": "Name",
    "listings.google_place_id": "GooglePlaceID",
    "listings.listing_status": "ListingStatus",
    "listings.listing_visible": "ListingVisible",
    "listing_info.description": "Description",
    "listing_info.info_visible": "InfoVisible",
    "listing_info.info_status": "InfoStatus",
    "contact_info.name": "ContactName",
    "contact_info.email": "Email",
    "contact_info.local_phone": "LocalPhone",
    "contact_info.international_phone": "InternationalPhone",
    "locations.address1": "Address1",
    "locations.address2": "Address2",
    "locations.city": "City",
    "locations.state": "State",
    "locations.zip": "Zip",
    "locations.country": "Country",
    "locations.lat": "Lat",
    "locations.lng": "Lng",
    "listing_images.image_1": "Image1",
    "listing_images.image_2": "Image2",
    "opening_dates.start_date_1": "StartDate1",
    "opening_dates.end_date_1": "EndDate1",
    "opening_dates.start_date_2": "StartDate2",
    "opening_dates.end_date_2": "EndDate2",
    "opening_hours.mon_open": "MonOpen",
    "opening_hours.mon_close": "MonClose",
    "bridge_attributes_listings.attribute_1": "Attribute1",
    "bridge_attributes_listings.attribute_2": "Attribute2"
  }
}


## Transform stage

-	Converts raw imported data into a consistent, normalized, and structured format.
-	Prepares fields for deduplication, hashing, and staging insertion.
-	Ensures that the mapping from CSV/API columns ‚Üí database columns is applied.
-	Applies optional normalization and rounding for comparison and hash keys.

1. **Field Mapping** ‚Äì CSV/API ‚Üí DB tables (multi-table).
2. **Normalization** ‚Äì Trim, lowercase, remove special chars.
3. **Lat/Lng Rounding** ‚Äì For dedupe consistency.
4. **Hash Fields & Dedupe Key** ‚Äì Generate staging `dedupe_key`.
5. **Deterministic Internal IDs** ‚Äì Map multiple external IDs to `listing_id`.
6. **Output** ‚Äì Transformed file ready for staging load.

### Key Sub-Steps in Transform

a) Field Mapping
-	Maps source columns to database columns.

Example:
"map_fields": {
  "listings.name": "Name",
  "locations.address1": "Street Address",
  "contact_info.email": "Email"
}

Effect:
-	Name in CSV becomes listings.name in staging.
-	Ensures all tables are properly populated for multi-table imports.

‚∏ª

b) Normalization
	‚Ä¢	Cleans and standardizes fields:
	‚Ä¢	Lowercase / uppercase names or emails
	‚Ä¢	Remove extra whitespace
	‚Ä¢	Strip special characters
	‚Ä¢	Example:

"normalize_fields": ["listings.name", "locations.address1"]

	‚Ä¢	Effect:
	‚Ä¢	Makes deduplication and hashing consistent.
	‚Ä¢	Reduces false duplicates caused by formatting differences.

‚∏ª

c) Lat/Lng Rounding
	‚Ä¢	Round coordinates for deduplication:

	"round_lat_lng": 5

		‚Ä¢	Effect:
	‚Ä¢	lat/lng rounded to 5 decimal places.
	‚Ä¢	Used in dedupe key and hash calculation.
	‚Ä¢	Original lat/lng preserved for mapping and production.

‚∏ª

d) Hash Fields
	‚Ä¢	Compute hash of selected fields to generate dedupe keys:

	"hash_fields": ["listings.name", "locations.address1", "lat_rounded", "lng_rounded"]

	‚Ä¢	Effect:
	‚Ä¢	Creates a unique string (hash) for detecting duplicates.
	‚Ä¢	Used in staging dedupe and optionally for logging.

‚∏ª

e) Dedupe Key Generation
-	Combines normalized and hashed fields to form dedupe_key:
	
"dedupe_key_fields": ["listings.name", "locations.address1", "lat_rounded", "lng_rounded"]

Effect:
-	One canonical key per logical entity.
-	Helps identify duplicates within the import batch and later during promotion.

‚∏ª

3. Output of Transform Stage

After transform:
-	All staging columns are filled, with standardized, mapped, and normalized values.
-	dedupe_key and hash fields are ready for duplicate detection.
-	Audit fields (imported_from, created_at, updated_at) can be added now.
-	Transformed file is saved to S3 and ready for validation stage.

‚∏ª

4. Notes / Best Practices
	1.	Mapping comes first, normalization after mapping:
	‚Ä¢	You normalize the mapped DB fields, not raw CSV column names.
	2.	Do not round or hash before normalization:
	‚Ä¢	Otherwise, formatting inconsistencies might create different hash keys.
	3.	Staging table receives all fields, including transform-generated fields:
	‚Ä¢	Raw fields
	‚Ä¢	Normalized fields
	‚Ä¢	Rounded lat/lng
	‚Ä¢	Hash/dedupe keys

‚∏ª

üí° Summary:
Transform stage = raw import ‚Üí clean, normalized, mapped, dedupe-ready data.
	‚Ä¢	It prepares everything needed for validation and later staging without touching production yet.
	‚Ä¢	Outputs transformed file that serves as input to validation stage.

---

## Validation Stage

### Validation Stage Highlights

Purpose of the Validation Stage

The validation stage is the "data quality assurance" step:
	‚Ä¢	Validates transformed data against data types, business rules, and required fields.
	‚Ä¢	Computes data quality scores and identifies problematic records.
	‚Ä¢	Separates valid records from invalid ones for downstream processing.
	‚Ä¢	Outputs validated file containing only records that pass all validation checks.

1. **Data Type Validation** ‚Äì Ensure fields match expected types (numeric, email, phone, etc.).
2. **Required Field Checks** ‚Äì Verify all mandatory fields are present and non-empty.
3. **Business Rule Validation** ‚Äì Apply DMO-specific constraints and geographic bounds.
4. **Quality Scoring** ‚Äì Compute completeness, accuracy, and consistency scores.
5. **Error Reporting** ‚Äì Log validation failures with detailed error messages.
6. **Output** ‚Äì Validated file ready for staging load.

### Key Sub-Steps in Validation

a) Data Type Validation
	‚Ä¢	Validate field types against expected formats:
	‚Ä¢	Lat/lng must be numeric and within valid ranges
	‚Ä¢	Email addresses must match email format
	‚Ä¢	Phone numbers must match phone format
	‚Ä¢	Dates must be valid date formats
	‚Ä¢	Example:

"data_types": {
  "locations.lat": "numeric",
  "locations.lng": "numeric", 
  "contact_info.email": "email",
  "contact_info.local_phone": "phone"
}

	‚Ä¢	Effect:
	‚Ä¢	Invalid data types are flagged and excluded from validated output.
	‚Ä¢	Prevents downstream errors during staging and promotion.

‚∏ª

b) Required Field Checks
	‚Ä¢	Verify all mandatory fields are present:
	‚Ä¢	Example:

"required_fields": [
  "listings.name",
  "locations.address1",
  "locations.city"
]

	‚Ä¢	Effect:
	‚Ä¢	Records missing required fields are flagged for manual review.
	‚Ä¢	Ensures data completeness before staging.

‚∏ª

c) Business Rule Validation
	‚Ä¢	Apply DMO-specific constraints:
	‚Ä¢	Geographic bounds (lat/lng within DMO territory)
	‚Ä¢	Category restrictions
	‚Ä¢	Custom business logic
	‚Ä¢	Example:

"business_rules": {
  "lat_bounds": [-90, 90],
  "lng_bounds": [-180, 180],
  "dmo_geographic_bounds": {
    "min_lat": 40.0,
    "max_lat": 45.0,
    "min_lng": -80.0,
    "max_lng": -70.0
  }
}

	‚Ä¢	Effect:
	‚Ä¢	Records violating business rules are flagged for review.
	‚Ä¢	Maintains data integrity and DMO-specific requirements.

‚∏ª

d) Quality Scoring
	‚Ä¢	Compute data quality metrics:
	‚Ä¢	Completeness: percentage of fields populated
	‚Ä¢	Accuracy: validation of field formats and values
	‚Ä¢	Consistency: internal consistency checks
	‚Ä¢	Example:

"quality_scoring": {
  "enabled": true,
  "weights": {
    "completeness": 0.4,
    "accuracy": 0.3,
    "consistency": 0.3
  }
}

	‚Ä¢	Effect:
	‚Ä¢	Provides quantitative measure of data quality.
	‚Ä¢	Helps prioritize records for manual review.

‚∏ª

e) Error Reporting
	‚Ä¢	Log validation failures with detailed information:
	‚Ä¢	Record identifier
	‚Ä¢	Field that failed validation
	‚Ä¢	Validation rule that was violated
	‚Ä¢	Suggested correction
	‚Ä¢	Effect:
	‚Ä¢	Enables debugging and data correction.
	‚Ä¢	Stored in pipeline.job_errors for analysis.

‚∏ª

3. Output of Validation Stage

After validation:
	‚Ä¢	Validated file contains only records that pass all validation checks.
	‚Ä¢	Quality scores are computed for each record.
	‚Ä¢	Validation errors are logged in pipeline.job_errors.
	‚Ä¢	Validated file is ready for staging load.

‚∏ª

4. Notes / Best Practices
	1.	Fail fast for critical errors:
	‚Ä¢	Stop processing immediately for data type violations.
	‚Ä¢	Continue processing for warnings and optional issues.
	2.	Comprehensive error reporting:
	‚Ä¢	Include record context and suggested corrections.
	‚Ä¢	Enable efficient debugging and data correction.
	3.	Quality scoring:
	‚Ä¢	Use consistent scoring methodology across all DMOs.
	‚Ä¢	Provide actionable insights for data improvement.

‚∏ª

üí° Summary:
Validation stage = transformed data ‚Üí quality-assured, validated data.
	‚Ä¢	It ensures only high-quality data proceeds to staging and production.
	‚Ä¢	Provides comprehensive error reporting and quality metrics.



# how staging -> public promotion works


1. Staging contains everything
	‚Ä¢	Every record from your import (CSV, API, or other source) goes into the staging tables, even if:
		‚Ä¢	It‚Äôs a duplicate within the batch
		‚Ä¢	It might overwrite production data
		‚Ä¢	It‚Äôs missing some fields
	‚Ä¢	Staging is essentially a ‚Äúsandbox‚Äù:
		‚Ä¢	You can review, clean, normalize, and flag records before they affect production.
		‚Ä¢	All audit info (imported_from, created_at, updated_at) is stored here.

‚∏ª

2. Promotion selects only some rows
	‚Ä¢	Promotion is conditional, based on:
		‚Ä¢	ok_to_promote / status_field
		‚Ä¢	Deduplication against production (dedupe_key comparison)
		‚Ä¢	Merge/conflict rules (merge_policy)
		‚Ä¢	Manual override flags (manually_modified)
	‚Ä¢	Only rows that pass these checks are inserted or updated in production.

‚∏ª

3. Example flow
	1.	Import ‚Üí Staging
	‚Ä¢	All rows go in.
	‚Ä¢	Compute dedupe_key, normalize, round lat/lng, hash fields.
	‚Ä¢	Flag duplicates within staging, but keep all rows.
	2.	Staging review
	‚Ä¢	Optional human review or automated checks.
	‚Ä¢	Rows marked ok_to_promote = false stay in staging.
	3.	Promotion ‚Üí Production
	‚Ä¢	Compute dedupe key on production data.
	‚Ä¢	Compare staging rows to production: decide insert vs update vs skip.
	‚Ä¢	Apply merge_policy and respect manually_modified flags.
	‚Ä¢	Update bridge tables if needed (update_bridge_table = true).
	4.	Post-promotion
	‚Ä¢	Staging table keeps all rows for auditing.
	‚Ä¢	Production only contains clean, final, promoted rows.

‚∏ª

‚úÖ Rule of thumb:
	‚Ä¢	Staging = full import, with audit + dedupe info + review flags
	‚Ä¢	Production = clean, deduplicated, conflict-resolved data


‚∏ª

## S3 File Naming

### raw, transformed, and validated file names
s3://{environment: dev, staging, app}-trippl-data/{stage: raw, transformed, validated}/{data-domain: listings, reviews, events}/{country: xx}/{state/province: xx}/{dmo-id}/{date: YYYYMMDD}/{batch: nnn}-{timestamp: YYYYMMDDTHHMMSS}.{format: json, csv, xls}.gz

### image names
s3://{environment: dev, staging, app}-trippl-images/{country}/{state}/{dmo-id}/{listing-id}/{filename}.{ext}


## Admin pages
add permissions for new pipeline functions. add all permissions to the super_admin role.

all Pipeline menus and pages should be protected on the front and back end with required permission pipeline:read.

make a new sidebar menu section called Pipeline, visible to users with permission

make admin pages to manage all steps of the pipeline. 

admin pages should follow the structure of existing admin pages (topbar, sidebar etc)


## Job Scheduling
use manual scheduling/execution on localhost.

on dev/staging/app use aws SQS to schedule and execute jobs in the job_queue

**if dmo config changes** then add a dialog at save time that checks for scheduled jobs and lets user ignore or cancel.  Changes to schedule shouldn't be an issue, though an old job with old config might overwrite new job(s).


## suggested Pipeline Schema table design

```sql

example config_params:
"import": {
  "source_type": "csv",   -- 'csv', 'wordpress', 'drupal', 'website', 'api', 'eventbrite'
  "file_format": "csv",   -- 'csv', 'json'
  "delimiter": ",",
  "encoding": "utf-8",
  "skip_header": true
}

// do the field mapping in the transform step, not the load-to-staging step.
// That way, your staging table always looks like your internal canonical schema (name, address, lat, lng, etc.), regardless of source.

"transform": {
  "map_fields": {
    "listings.name": "Name",
    "locations.address1": "Street Address",
    "contact_info.email": "Email"
  },
  "normalize_fields": [
    "listings.name",
    "locations.address1",
    "listings.category"
  ],
  "round_lat_lng": 5,
  "hash_fields": ["listings.name", "locations.address1", "lat", "lng", "listings.category"],
  "dedupe_key_fields": ["listings.name", "locations.address1", "lat", "lng"]
}

"validation": {
  "data_types": {
    "locations.lat": "numeric",
    "locations.lng": "numeric",
    "contact_info.email": "email",
    "contact_info.local_phone": "phone"
  },
  "required_fields": [
    "listings.name",
    "locations.address1",
    "locations.city"
  ],
  "business_rules": {
    "lat_bounds": [-90, 90],
    "lng_bounds": [-180, 180],
    "dmo_geographic_bounds": {
      "min_lat": 40.0,
      "max_lat": 45.0,
      "min_lng": -80.0,
      "max_lng": -70.0
    }
  },
  "quality_scoring": {
    "enabled": true,
    "weights": {
      "completeness": 0.4,
      "accuracy": 0.3,
      "consistency": 0.3
    }
  }
}

"staging": {
  "match_on": [
    "listings.dedupe_key",
    "locations.dedupe_key",
    "contact_info.dedupe_key"
  ],
  "conflict_resolution": "update_if_changed",
  "status_field": {
    "listings": "ok_to_promote",
    "locations": "ok_to_promote",
    "contact_info": "ok_to_promote"
  },
  "audit_fields": ["created_at", "updated_at", "imported_from"]
}

"promotion": {
  "merge_policy": "latest_wins", // if a row exists then staging version overwrites it
  // "merge_policy": "skip_if_exists" // keep production data if it already exists
  // "merge_policy": "field_level_merge" // update only fields that changed
  "update_bridge_table": true  // using a bridge table for many-to-many relationships, set to true to ensure relationships are updated during promotion. If false, only the main tables (listings, locations) are updated, but bridge/assocaiation tables are left stale (?)
  "audit_fields": ["promoted_at", "promoted_by", "imported_from"],
  "conflict_resolution": {
    "listings": "latest_wins",
    "locations": "field_level_merge",
    "contact_info": "latest_wins"
  }
}

    <!-- ******************** -->
      <!-- PIPELINE SCHEMA      -->
      <!-- ******************** -->

      -- Create pipeline schema
      CREATE SCHEMA IF NOT EXISTS pipeline;

      -- Create enum type
      CREATE TYPE pipeline.target_type AS ENUM ('listings', 'listing_info');

      -- Bridge Source Target table (lookup only)
      CREATE TABLE pipeline.bridge_source_target (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        source_id TEXT NOT NULL,     -- like Priceline id, or source url, or computed (hash(dmo_id+name+city))
        target_id INT,               -- id of listings or listing_info record
        target_type pipeline.target_type NOT NULL,   -- e.g. 'listings', 'listing_info'
        certainty_score NUMERIC(5,4) DEFAULT 1.0,    -- confidence score for the match
        mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT unique_dmo_source_target UNIQUE (dmo_id, source_id, target_type)
      );

      -- Import Configs table
      CREATE TABLE pipeline.configs (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        name VARCHAR NOT NULL,

        data_domain VARCHAR NOT NULL, -- listings, events, reviews
        source_type VARCHAR NOT NULL, -- csv, wordpress, drupal, website, api, eventbrite
        source_url TEXT,
        schedule VARCHAR,
        config JSONB NOT NULL,
        field_mappings JSONB NOT NULL,
        validation_rules JSONB NOT NULL,
        active BOOLEAN NOT NULL DEFAULT TRUE,
        version INT DEFAULT 1,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT unique_dmo_source_name UNIQUE (dmo_id, name)
      );

      -- Create enum type
      CREATE TYPE pipeline.pipeline_stage AS ENUM ('import', 'transform', 'validate', 'stage', 'review', 'promote');

      -- Jobs table
      CREATE TABLE pipeline.jobs (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        current_stage pipeline.pipeline_stage NOT NULL,
        status VARCHAR DEFAULT 'pending', -- pending, running, completed, failed, cancelled

        triggered_by VARCHAR,
        trigger_type VARCHAR, -- e.g., manual / schedule / system
        records_total INT DEFAULT 0,
        records_new INT DEFAULT 0,
        records_updated INT DEFAULT 0,
        records_deleted INT DEFAULT 0,
        records_duplicates INT DEFAULT 0,
        records_unchanged INT DEFAULT 0,
        records_failed INT DEFAULT 0,
        records_warning INT DEFAULT 0,

        enqueued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        started_at TIMESTAMP,
        finished_at TIMESTAMP
      );

      -- Job Stages table
      CREATE TABLE pipeline.job_stages (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        name pipeline.pipeline_stage NOT NULL,
        order_index INT NOT NULL,
        status VARCHAR DEFAULT 'pending', -- pending, running, completed, failed, cancelled

        attempt_count INT DEFAULT 0, -- current attempt number
        retry_count INT DEFAULT 0, -- number of times the stage has been retried
        next_attempt_at TIMESTAMP, -- next time the stage will be attempted
        locked_by VARCHAR, -- user/worker who locked the stage
        locked_at TIMESTAMP, -- time the stage was locked

        message TEXT,
        result_summary JSONB,

        enqueued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        started_at TIMESTAMP,
        finished_at TIMESTAMP,
        CONSTRAINT unique_job_stage UNIQUE (job_id, name)
      );

      -- Config Snapshots table
      CREATE TABLE pipeline.config_snapshots (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        name VARCHAR NOT NULL,

        data_domain VARCHAR NOT NULL, -- listings, events, reviews
        source_type VARCHAR NOT NULL, -- csv, wordpress, drupal, website, api, eventbrite
        source_url TEXT,
        schedule VARCHAR,
        config JSONB NOT NULL,
        field_mappings JSONB NOT NULL,
        validation_rules JSONB NOT NULL,
        version INT DEFAULT 1,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

      -- Files table
      CREATE TABLE pipeline.files (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NULL REFERENCES pipeline.jobs(id),
        stage_name VARCHAR NOT NULL, -- import, transform, validate

        file_path VARCHAR NOT NULL,
        data_domain VARCHAR NOT NULL, -- listings, events, reviews
        country VARCHAR(2),
        file_date DATE,

        batch INT GENERATED ALWAYS AS IDENTITY,  -- for sequential numeric batch IDs
        format VARCHAR,
        file_size BIGINT,
        record_count INT,
        file_hash VARCHAR,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

     -- Job Results table: summary of results by stage
      CREATE TABLE pipeline.stage_results (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        job_stages_id INT NOT NULL REFERENCES pipeline.job_stages(id),

        -- Lifecycle and outcome
        status VARCHAR NOT NULL, -- current progress or outcome: pending, running, completed, failed,cancelled

        records_total INT DEFAULT 0,
        records_new INT DEFAULT 0,
        records_updated INT DEFAULT 0,
        records_deleted INT DEFAULT 0,
        records_duplicates INT DEFAULT 0,
        records_unchanged INT DEFAULT 0,
        records_failed INT DEFAULT 0,
        records_warning INT DEFAULT 0,

        -- Timestamps - stage specific
        started_at TIMESTAMP NULL,
        finished_at TIMESTAMP NULL
      );

      -- Job Errors table: detailed errors by record
      --   jobs can complete but still have errors. Errors don't mean a job/stage failed. Failed is a bigger deal.
      CREATE TABLE pipeline.job_errors (
        id SERIAL PRIMARY KEY,
        dmo_id INT NOT NULL REFERENCES public.dmos(id),
        job_id INT NOT NULL REFERENCES pipeline.jobs(id) ON DELETE CASCADE,
        job_stages_id INT NOT NULL REFERENCES pipeline.job_stages(id),

        file_id INT REFERENCES pipeline.files(id),
        record_id VARCHAR,
        original_record JSONB,
        error_type TEXT,
        error_message TEXT,

        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );

```

## Suggesting Staging Schema

**Schema name:** staging
-	Stores all imported/transformed rows before promotion.
-	Keeps full audit trail, dedupe keys, hashes, and flags.
-	All raw fields are preserved, along with normalized/hashed versions for deduplication.

```sql

-- Listings table (staging)
CREATE TABLE staging.listings (
    id serial PRIMARY KEY,
    dmo_id integer NULL,
    name character varying NULL,
    google_place_id character varying NULL,
    location_id integer NULL,
    listing_visible boolean DEFAULT true,
    listing_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    -- dedupe/audit fields
    dedupe_key character varying NOT NULL,
    hash_name character varying,
    hash_address character varying,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now(),
    ok_to_promote boolean DEFAULT false
);

-- Listing Info (staging)
CREATE TABLE staging.listing_info (
    id serial PRIMARY KEY,
    listing_id integer NULL,
    description character varying NULL,
    contact_info_id integer NULL,
    info_visible boolean DEFAULT true,
    info_status character varying NULL,
    manually_modified boolean DEFAULT false,
    manually_modified_at timestamp NULL,
    manually_modified_by integer NULL,
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now(),
    ok_to_promote boolean DEFAULT false
);

-- Contact Info (staging)
CREATE TABLE staging.contact_info (
    id serial PRIMARY KEY,
    name character varying NULL,
    email character varying NULL,
    local_phone character varying NULL,
    international_phone character varying NULL,
    website_url character varying NULL,
    facebook_url character varying NULL,
    instagram_url character varying NULL,
    youtube_url character varying NULL,
    tiktok_url character varying NULL,
    twitter_url character varying NULL,
    pinterest_url character varying NULL,
    tripadvisor_url character varying NULL,
    yelp_url character varying NULL,
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Locations (staging)
CREATE TABLE staging.locations (
    id serial PRIMARY KEY,
    address1 character varying NULL,
    address2 character varying NULL,
    city character varying NULL,
    state character varying NULL,
    zip character varying NULL,
    country char(2) NULL,
    lat numeric(10,8) NULL,
    lng numeric(11,8) NULL,
    lat_rounded numeric(10,5),
    lng_rounded numeric(11,5),
    dedupe_key character varying NOT NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Listing Images (staging)
CREATE TABLE staging.listing_images (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    image character varying NULL,
    rank integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Dates (staging)
CREATE TABLE staging.opening_dates (
    id serial PRIMARY KEY,
    listing_info_id integer NULL,
    start_date character varying NULL,
    end_date character varying NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Opening Hours (staging)
CREATE TABLE staging.opening_hours (
    id serial PRIMARY KEY,
    opening_date_id integer NULL,
    day_id integer NULL,
    open_time character varying NULL,
    close_time character varying NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);

-- Bridge tables (staging)
CREATE TABLE staging.bridge_attributes_listings (
    id serial PRIMARY KEY,
    attribute_id integer NULL,
    listing_id integer NULL,
    imported_from character varying,
    created_at timestamp DEFAULT now(),
    updated_at timestamp DEFAULT now()
);
```

**Notes on staging:**
-	dedupe_key is always computed using a hash of normalized key fields (e.g., name + address + lat/lng).
-	ok_to_promote is false by default, set to true only after review.
-	All imported fields are kept to allow audit and traceability.

## Suggested Public Schema Updates
```sql
     <!-- ****************************** -->
      <!-- PUBLIC SCHEMA HASH FIELDS -->
      <!-- ****************************** -->

      -- Add hash fields to public schema tables for pipeline tracking

      -- Listings table hash fields
      ALTER TABLE public.listings
      ADD COLUMN listing_match_hash CHARACTER VARYING(64),
      ADD COLUMN listing_content_hash CHARACTER VARYING(64);

      -- Listing Info table hash fields
      ALTER TABLE public.listing_info
      ADD COLUMN listing_info_content_hash CHARACTER VARYING(64),
      ADD COLUMN images_content_hash CHARACTER VARYING(64),
      ADD COLUMN dates_content_hash CHARACTER VARYING(64),
      ADD COLUMN hours_content_hash CHARACTER VARYING(64),
      ADD COLUMN attributes_content_hash CHARACTER VARYING(64);

      -- Locations table hash fields
      ALTER TABLE public.locations
      ADD COLUMN location_content_hash CHARACTER VARYING(64);

      -- Contact Info table hash fields
      ALTER TABLE public.contact_info
      ADD COLUMN contact_info_content_hash CHARACTER VARYING(64);
```
**Notes on production public schema:**
-	Only reviewed and promoted rows live here.
-	Deduplication is applied during promotion; no dedupe keys stored unless needed for performance.
-	promoted_at and promoted_by track the exact promotion event.
-	manually_modified flags ensure pipeline doesn‚Äôt overwrite manual edits.

‚úÖ Key Design Points for Enterprise-Grade Traceability
1.	Staging = full audit, full imported data, dedupe & normalized fields
2.	Production = clean, promoted data, audit of promotion + manual edits
3.	Multi-table mapping is explicit: listings.name, contact_info.email, locations.address1
4.	Deduplication & hashing: staging generates dedupe_key per record; production dedupe is computed on-the-fly during promotion
5.	Audit fields everywhere: imported_from, created_at, updated_at, promoted_at, promoted_by
6.	Bridge tables follow the same pattern for relational mapping, with promotion/audit fields


# Step by step parent/sub-table deduplication, hashing, promotion, and matching logic. 

# Listings Pipeline: Parent and Sub-Table Promotion

## 1. Parent Tables

| Table | Key Fields / Dedupe Key | Promotion Flag |
|-------|------------------------|----------------|
| `listings` | `name + address + lat/lng` | `ok_to_promote` |
| `listing_info` | `description + contact_info_id` | `ok_to_promote` |

- `dedupe_key` is computed in staging from key identifying fields.  
- Only rows with `ok_to_promote = true` are eligible for promotion.  
- Matching during promotion is done against production tables using `dedupe_key`.

---

## 2. Sub-Tables

Promotion of sub-tables is **conditional on parent approval**. Each sub-table has its own dedupe key used for matching against production.

| Sub-Table | Key Fields for Hash / Dedupe | Linked To | Notes on Matching / Promotion |
|-----------|-----------------------------|-----------|-------------------------------|
| `contact_info` | `email + phone + name` | `listing_info.contact_info_id` | Reuse existing row in production if dedupe key matches; else insert new. |
| `locations` | `address1 + city + state + zip + country` | `listings.location_id` | Same: match dedupe key, reuse or insert new. |
| `listing_images` | `listing_info_id + image_url` | `listing_info.id` | Avoid duplicate images per listing. |
| `opening_dates` | `listing_info_id + start_date + end_date` | `listing_info.id` | Prevent duplicate date ranges for same listing_info. |
| `opening_hours` | `opening_date_id + day_id + open_time + close_time` | `opening_dates.id` | Match exact schedule; parent opening_date must exist. |
| `bridge_attributes_listings` | `listing_id + attribute_id` | `listings.id` | Deduplicate the pair of foreign keys; parent listing must be promoted. |

---

## 3. Promotion Flow

1. Compute **dedupe keys** for all parent and sub-table staging rows.  
2. Review parent rows and set `ok_to_promote`.  
3. For each parent row approved:
   - Promote parent row to production.  
   - For each linked sub-table:
     - Check if `dedupe_key` exists in production.
     - **If exists:** reuse production ID.  
     - **If not exists:** insert new row with audit fields.  
   - Update parent table foreign keys to point to promoted sub-table IDs.  
4. Maintain audit fields throughout:
   - `imported_from`  
   - `created_at`, `updated_at`  
   - `promoted_at`, `promoted_by`  

---

## 4. Key Principles

- Sub-tables **do not promote independently**; promotion is controlled by parent `ok_to_promote`.  
- Deduplication ensures **no duplicate contact_info, locations, or images** across multiple listings.  
- Hash/dedupe keys in staging provide **reliable matching** during promotion.  
- Bridge tables follow the same dedupe/matching rules as sub-tables.  
- All audit fields are preserved for **enterprise-grade traceability**.

# Stage step

## Finding matches

**find listing**
- listing_id = select target_id from bridge_source_target(source_id, source_type='listing') - todo: we only lookup listings by source_id and 'listing' type. Then if found we should verify that the dmo_id is actually the import.dmo_id.
- if not found
- listing_id = select listing_id from public.listings where public.listings.listing_match_hash = generated_match_hash_from_import_data()
- if not found
- new listing record

**find listing_info**
- NOTE: we never really have to lookup listing_info by it's content. Once we have found the listing, and we have the import dmo_id, then a listing_info record exists for that listing and dmo_id (check for content changes), or it doesn't (create new).
- todo: figure out if we need to do a verification with a listings_info_match_hash.
- todo: also figure out how to handle duplicate records that may have different information: which is used for the update?  I guess we add to the staging table as duplicates and force checking which one is the updating record source.
- 
- if new listing record, then also new listing_info record
- (there really shouldn't be any match in the db, should we double check to catch unexpected results?)
- if listing_id was found
- listing_info_id = select target_id from bridge_source_target(source_id, source_type='listing_info', dmo_id=import_dmo_id) (this is where parent id could be used instead of dmo_id to make sure we have the right listing_info)
- if not found, then we make sure we are not just missing the mapping, lookup listing_info by listing_id and dmo_id.
- if not found (which it shouldn't be)
- create new listing_info

## dedupe_key in staging
Exists only in staging tables (you don‚Äôt store it in production).
	‚Ä¢	It‚Äôs generated on-the-fly during the transform step, usually by hashing a combination of normalized fields, e.g.:

hash(name_normalized + address_normalized + lat_rounded + lng_rounded)	

	‚Ä¢	Purpose: to detect duplicates across imports and multiple sources before promotion.

‚∏ª

2. Comparing to production
	‚Ä¢	During the staging ‚Üí production promotion step, the staging dedupe_key is compared to an on-the-fly dedupe key generated for production rows.
	‚Ä¢	Production rows don‚Äôt store dedupe_key, but you recompute it using the same hashing logic on production data fields (name, address, lat/lng, etc.).
	‚Ä¢	If a match is found:
	‚Ä¢	Conflict resolution is triggered (e.g., latest_wins, field_level_merge, skip_if_exists).
	‚Ä¢	If no match is found:
	‚Ä¢	Staging row is inserted as a new row in production.

‚∏ª

3. Why not store dedupe_key in production?
	‚Ä¢	Keeps production schema clean ‚Äî no temporary hash fields.
	‚Ä¢	Avoids stale dedupe keys if production data is edited manually (manually_modified).
	‚Ä¢	Staging handles all deduplication logic, so production only sees final clean rows.

‚∏ª

4. Optional enhancements
	‚Ä¢	Some pipelines do store dedupe_key in production for speed, but then:
	‚Ä¢	You need to recompute it anytime a row is manually modified, which adds complexity.
	‚Ä¢	Usually easier and safer to compute it on-the-fly during promotion.

‚∏ª
