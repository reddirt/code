1. Deterministic Internal IDs
	•	When are internal IDs computed in Transform? → After field mapping, normalization, rounding lat/lng, and hashing; before staging.
	•	How do we handle conflicts when multiple sources map to the same internal ID? → Multiple sources can map to the same listing (tracked in bridge_source_listing). Only one source can map to a specific listing_info (enforced via bridge_source_listinginfo); conflicts at this level must be flagged and resolved. more details: {
        1.	Listings level (bridge_source_listing)
        •	Multiple sources can map to the same listing_id.
        •	Conflicts are expected and allowed; the bridge table tracks all source contributions.
        •	No conflict resolution needed at this level unless the sources disagree on critical fields used to generate the deterministic internal ID — in that case, flag in staging/review for manual inspection.
        2.	Listing info level (bridge_source_listinginfo)
        •	Only one source should map to a given listing_info record.
        •	If two sources attempt to map to the same listing_info_id, treat it as a conflict:
        •	Detect in Transform/Validation stage.
        •	Flag for manual review.
        •	Optionally, apply a priority rule (e.g., prefer DMO over scraped sources) to auto-resolve.
        3.	Multiple listing_info per listing (listing_info.listing_id FK)
        •	Allowed and expected.
        •	Each listing_info record is uniquely identified (internal ID or hash of relevant fields).
        •	Conflicts are only when two sources claim the same listing_info ID, not when multiple listing_info rows exist for the same listing.

        Summary Rule:
        •	Multiple sources → same listing: allowed, tracked in bridge_source_listing.
        •	Multiple sources → same listing_info: not allowed, must be flagged/resolved.
        •	Multiple listing_info per listing: allowed, as long as IDs are unique.
    }
	•	Should we use a deterministic hash of source fields, or a different approach? → Yes, use a deterministic hash of normalized fields (name, address, lat/lng). Source IDs are not the same as internal IDs — source IDs identify the external record; internal IDs identify the canonical listing in our system.

2. Field mapping configuration
	•	How do we handle dynamic CSV headers that vary by DMO? → Per-DMO field mapping configuration.
	•	Should we support auto-detection of common field patterns? → Yes, as a fallback.
	•	How do we validate mappings before processing? → Check for required fields; warn or fail if missing critical data.

3. Geocoding integration
	•	Which geocoding service should we use (Google, Mapbox, etc.)? → Google Places API (see example as in /api-server/src/scripts/GoogleImport).
	•	How do we handle rate limits and failures? → Async queue, retries, caching.
	•	Should geocoding be synchronous or asynchronous in Transform? → Asynchronous. Also, geocoding only happens if no match is found by source_id; otherwise we reuse existing lat/lng and address from locations record.

4. Error handling and recovery
	•	What happens if Transform fails partway through a large file? → Log row-level errors; allow retry of failed rows.
	•	Should we support partial promotion (some rows succeed, others fail)? → Yes.
	•	How do we handle schema changes between staging and production? → Version schemas; run migrations; reject promotion on incompatibility.

5. Performance and scalability
	•	What are expected file sizes and record counts? → 	•	up to 500 records or 5Mb .cvs/.json, with streaming/chunked processing to create sequentially numbered batch files.
	•	Should we support streaming/chunked processing for large files? → Yes
	•	How do we handle concurrent jobs for the same DMO? → Queue per DMO; lock staging per import.

6. Admin UI requirements
	•	What level of detail should the Review interface show? → Show mappings, dedupe keys, conflicts, and source vs canonical data.
	•	Should we support bulk operations (approve all, reject all)? → Yes.
	•	How do we display conflicts and resolution options? → Highlight differences; allow manual choice or priority rules.

⸻

Suggested improvements (decisions)
	1.	Add validation stage → Insert between Transform and Stage. Validate:
	•	Data types
	•	Required fields
	•	Business rules (e.g., lat/lng within bounds)
	•	Data quality scoring (optional for Phase 2)
	2.	Enhanced error reporting → Log row-level errors, summarize in admin UI, alert on bulk failures.
	3.	Configuration versioning → Store per-DMO pipeline config versions; allow rollback to prior mapping rules.
	4.	Performance monitoring → Track processing time, row counts, geocoding requests, queue backlog.
	5.	Rollback capability → Track promoted batches; allow atomic rollback; preserve staging data.

⸻

Implementation priorities
	•	Phase 1 (MVP) → Manual execution, CSV import, basic deduplication, core Review interface, manual promotion.
	•	Phase 2 (Enhanced) → Job queue (SQS), advanced mapping/validation, geocoding, enhanced error handling, monitoring.
	•	Phase 3 (Enterprise) → Automated scheduling, advanced conflict resolution, rollback, API integrations, analytics.

⸻

Technical considerations
	1.	Database performance → Index dedupe keys, consider partitioning, archive old staging data.
	2.	File storage → Use S3 for raw/transformed files, compress large files, set lifecycle cleanup.
	3.	Security → RBAC for pipeline operations, audit logs, encryption in transit and at rest.

⸻

Questions for you (decisions)
	•	Expected import volume → Phase 1: 10k–50k rows; Phase 2: up to 1M+ rows.
	•	Geocoding service → Google Places API (keep current).
	•	Validation strictness → Fail fast for required fields; collect warnings for optional issues.
	•	Admin UI priority → Functionality first (display changes, conflicts, bulk approve/reject), polish later.
	•	DMO-specific constraints → Per-DMO field mapping and deduplication rules.