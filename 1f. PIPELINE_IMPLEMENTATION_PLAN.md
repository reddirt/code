# Listings Pipeline Implementation Plan

**Status**: Planning Phase  
**Created**: 2024-12-19  
**Last Updated**: 2024-12-19  

## Overview

This document outlines the implementation plan for the enterprise-grade listings pipeline with a powerful graphical interface for managing data pipeline operations.

## Implementation Phases

### **Phase 1: Foundation & Database Schema** ‚è≥

#### 1.1 Database Schema Implementation
- [ ] **Create pipeline schema** with tables:
  - [ ] `pipeline.dmo_configs` - DMO-specific pipeline configurations
  - [ ] `pipeline.job_queue` - Job scheduling and execution tracking
  - [ ] `pipeline.job_configs` - Configuration snapshots for reproducibility
  - [ ] `pipeline.files` - File tracking and metadata
  - [ ] `pipeline.job_results` - Processing statistics
  - [ ] `pipeline.job_errors` - Error logging and debugging
  - [ ] `pipeline.bridge_source_listing` - Source-to-listing mapping
  - [ ] `pipeline.bridge_source_listing_info` - Source-to-listing_info mapping


- [ ] **Create staging schema** with tables:
  - [ ] `staging.listings`, `staging.listing_info`, `staging.contact_info`
  - [ ] `staging.locations`, `staging.listing_images`, `staging.opening_dates`
  - [ ] `staging.opening_hours`, `staging.bridge_attributes_listings`

- [ ] **Update public schema** with audit fields:
  - [ ] Add `promoted_at`, `promoted_by`, `imported_from` to existing tables
  - [ ] Add `manually_modified`, `manually_modified_at`, `manually_modified_by` flags

#### 1.2 Permissions & RBAC
- [ ] **Add pipeline permissions** to `permissions.ts`:
  ```typescript
  PIPELINE_READ = 'pipeline:read',
  PIPELINE_CREATE = 'pipeline:create', 
  PIPELINE_UPDATE = 'pipeline:update',
  PIPELINE_DELETE = 'pipeline:delete',
  PIPELINE_EXECUTE = 'pipeline:execute'
  ```
- [ ] **Update super_admin role** to include all pipeline permissions

---

### **Phase 2: Backend API Implementation** ‚è≥

#### 2.1 Pipeline Services (following existing patterns)
- [ ] **`PipelineConfigService`** - DMO configuration management
- [ ] **`PipelineJobService`** - Job queue and execution management  
- [ ] **`PipelineTransformService`** - Data transformation logic
- [ ] **`PipelineValidationService`** - Data validation and quality scoring
- [ ] **`PipelineStagingService`** - Staging data management
- [ ] **`PipelinePromotionService`** - Staging to production promotion

#### 2.2 Pipeline DAO Layer
- [ ] **`PipelineConfigDao`** - Configuration CRUD operations
- [ ] **`PipelineJobDao`** - Job queue management
- [ ] **`PipelineFileDao`** - File metadata and tracking
- [ ] **`PipelineValidationDao`** - Validation error tracking
- [ ] **`StagingDao`** - Staging table operations
- [ ] **`BridgeSourceListingDao`** - Source mapping operations

#### 2.3 API Routes (following existing patterns)
- [ ] **`/api/v1/pipeline/configs`** - DMO configuration management
- [ ] **`/api/v1/pipeline/jobs`** - Job queue operations
- [ ] **`/api/v1/pipeline/files`** - File upload and management
- [ ] **`/api/v1/pipeline/validate`** - Validation operations and error reporting
- [ ] **`/api/v1/pipeline/staging`** - Staging data operations
- [ ] **`/api/v1/pipeline/promote`** - Promotion operations

---

### **Phase 3: Frontend Admin Interface** ‚è≥

#### 3.1 Admin Pages Structure
- [ ] **`/admin/pipeline`** - Main pipeline dashboard
- [ ] **`/admin/pipeline/configs`** - DMO configuration management
- [ ] **`/admin/pipeline/jobs`** - Job queue monitoring
- [ ] **`/admin/pipeline/validation`** - Validation results and error reporting
- [ ] **`/admin/pipeline/staging`** - Staging data review
- [ ] **`/admin/pipeline/files`** - File management

#### 3.2 Components (following atomic design)
- [ ] **Atoms**: `PipelineStatusBadge`, `PipelineProgressBar`, `PipelineActionButton`, `ValidationErrorBadge`
- [ ] **Molecules**: `PipelineJobCard`, `PipelineConfigForm`, `PipelineFileUploader`, `ValidationResultsCard`
- [ ] **Organisms**: `PipelineDashboard`, `PipelineValidationInterface`, `PipelineStagingTable`, `PipelineReviewInterface`

#### 3.3 API Integration
- [ ] **`PipelineApi`** - API client following existing patterns
- [ ] **`usePipelineJobs`**, **`usePipelineConfigs`**, **`usePipelineValidation`** - Custom hooks for data management
- [ ] **`usePipelinePermissions`** - Permission checking hook

---

### **Phase 4: Core Pipeline Logic** ‚è≥

#### 4.1 Import Stage
- [ ] **File upload handling** with S3 integration
- [ ] **CSV/JSON parsing** with streaming for large files
- [ ] **Basic validation** and error reporting
- [ ] **File metadata tracking** in `pipeline.files`

#### 4.2 Transform Stage
- [ ] **Field mapping** based on DMO configuration
- [ ] **Data normalization** (trim, lowercase, special chars)
- [ ] **Lat/lng rounding** for deduplication consistency
- [ ] **Hash generation** for dedupe keys
- [ ] **Deterministic internal ID** computation
- [ ] **Geocoding integration** (Google Places API)
- [ ] **Output transformed file** to S3 for validation stage

#### 4.3 Validation Stage
- [ ] **Data type validation** (lat/lng bounds, email format, phone format, etc.)
- [ ] **Required field checks** based on DMO configuration
- [ ] **Business rule validation** (DMO-specific constraints, geographic bounds)
- [ ] **Quality scoring** (completeness, accuracy, consistency)
- [ ] **Error reporting** with detailed messages and suggestions
- [ ] **Fail fast** for critical errors, collect warnings for optional issues
- [ ] **Output validated file** to S3 for staging stage
- [ ] **Log validation errors** in `pipeline.job_errors` table

#### 4.4 Staging Stage
- [ ] **Load validated data** from validated file into staging tables
- [ ] **Multi-table insertion** with proper foreign key relationships
- [ ] **Deduplication detection** within staging batch
- [ ] **Conflict flagging** for manual review
- [ ] **Audit field population** (imported_from, created_at, etc.)
- [ ] **Set initial promotion flags** (ok_to_promote = false by default)

#### 4.5 Review Stage
- [ ] **Staging data display** grouped by dedupe_key
- [ ] **Change highlighting** (additions, updates, deletions)
- [ ] **Manual approval interface** with bulk operations
- [ ] **Conflict resolution** tools
- [ ] **Quality score display** for informed decision making

#### 4.6 Promotion Stage
- [ ] **Production comparison** using on-the-fly dedupe keys
- [ ] **Merge policy application** (latest_wins, field_level_merge, skip_if_exists)
- [ ] **Bridge table updates** for many-to-many relationships
- [ ] **Audit trail maintenance** (promoted_at, promoted_by)

---

### **Phase 5: Job Queue & Scheduling** ‚è≥

#### 5.1 Job Queue Implementation
- [ ] **Local execution** for development (manual scheduling)
- [ ] **SQS integration** for production environments
- [ ] **Retry logic** with exponential backoff
- [ ] **Job locking** to prevent concurrent execution
- [ ] **Progress tracking** and status updates

#### 5.2 Error Handling & Recovery
- [ ] **Row-level error logging** in `pipeline.job_errors`
- [ ] **Partial promotion** support for failed rows
- [ ] **Retry mechanisms** for transient failures
- [ ] **Dead letter queue** for permanently failed jobs

---

### **Phase 6: Integration & Testing** ‚è≥

#### 6.1 Existing System Integration
- [ ] **DMO management** integration with existing DMO system
- [ ] **User authentication** using existing Auth0 patterns
- [ ] **Permission system** integration with existing RBAC
- [ ] **File storage** integration with existing S3 setup

#### 6.2 Testing Strategy
- [ ] **Unit tests** for all services and DAOs
- [ ] **Integration tests** for pipeline stages
- [ ] **End-to-end tests** for complete pipeline flows
- [ ] **Performance tests** for large file processing

---

## **Design Decisions** ‚úÖ

### **Core Implementation Decisions**
1. **Geocoding Service**: ‚úÖ Use Google Places API as in `/api-server/src/scripts/GoogleImport` for now. Consider Mapbox or AWS Location later if cost/quota becomes an issue.

2. **File Size Limits**: ‚úÖ Start with 500 records/5MB per file with chunked processing. Scale up later as needed.

3. **Staging Data Retention**: ‚úÖ Keep staging data for 30 days, then archive/delete. Long enough for review + rollback, but prevents table bloat.

4. **Existing Schema**: ‚úÖ Add audit fields (created_at, updated_at, import_batch_id, status) to current staging tables. No need for entirely new versions unless schema diverges later.

5. **Job Queue**: ‚úÖ Use SQS in production for reliability and concurrency; allow manual execution locally for easier debugging.

### **Configuration & Error Handling**
6. **DMO Configuration**: ‚úÖ Store in `pipeline.dmo_configs` as JSONB to allow flexible per-DMO mappings.

7. **Error Handling**: ‚úÖ Fail fast for critical errors (schema mismatch, missing required columns). Collect warnings (e.g., missing phone numbers) but still process.

8. **Admin UI Priority**: ‚úÖ Functionality first (review, approve/reject, conflict resolution). Polish can come later.

9. **Permissions**: ‚úÖ Extend existing RBAC with pipeline:* permissions (e.g., pipeline.import, pipeline.promote).

10. **File Storage**: ‚úÖ Use existing S3 setup for raw and transformed files. Add lifecycle policies (raw files expire after 90 days).

### **Validation & Quality Control**
11. **Validation Error Handling**: ‚úÖ Validation errors should not block the entire batch. Mark failed rows, continue with valid rows, but show a batch error summary.

12. **Quality Scoring**: ‚úÖ Start with a simple threshold (e.g., 70%). Records below threshold are not ok_to_promote. Make scoring rules configurable later.

13. **Business Rules**: ‚úÖ Store DMO-specific geographic bounds in pipeline.dmo_configs as GeoJSON. Validate that listings fall within bounds during validation.

14. **File Processing**: ‚úÖ Validation should work in chunks/streams to avoid loading entire files into memory.

15. **Error Recovery**: ‚úÖ Failed validation records should be reviewable and retryable after correction (not silently dropped).

### **Integration with Existing Codebase**
16. **Database Migrations**: ‚úÖ Add pipeline schema and tables via new Liquibase changelog files for consistency.

17. **API Versioning**: ‚úÖ Pipeline APIs should follow existing /api/v1/ pattern.

18. **Authentication**: ‚úÖ Pipeline operations should use existing Auth0 middleware and RBAC checks.

19. **Logging**: ‚úÖ Integrate with existing logging infrastructure (LogUtils), with pipeline-specific tags for traceability.

20. **Testing**: ‚úÖ Pipeline tests should follow existing Jest patterns (unit + integration). Mock external services (Google API, S3, SQS).

---

## **Implementation Notes**

### **Following Existing Patterns**
- **Backend**: Three-layer architecture (Routes ‚Üí Services ‚Üí DAO)
- **Frontend**: Atomic design methodology (atoms ‚Üí molecules ‚Üí organisms)
- **Database**: Follow existing schema patterns and naming conventions
- **API**: RESTful endpoints following existing patterns
- **Permissions**: Integrate with existing RBAC system

### **Key Design Principles**
- **Functional Programming**: Pure functions, immutability, composition
- **Separation of Concerns**: Clear layer boundaries and responsibilities
- **Enterprise-Grade**: Full audit trails, error handling, and traceability
- **Scalable**: Support for large files and concurrent processing

### **MVP Scope (Phase 1)**
- Manual execution only (local development)
- CSV import support (500 records/5MB limit)
- Basic field mapping and transformation
- Data type validation and required field checks
- Quality scoring with 70% threshold
- Basic deduplication
- Core review interface with error reporting
- Manual promotion
- 30-day staging data retention

### **Future Enhancements (Phase 2+)**
- Job queue with SQS (production deployment)
- Advanced mapping/validation with business rules
- Automated geocoding with Google Places API
- Advanced quality scoring and analytics
- Geographic bounds validation with GeoJSON
- Enhanced error handling and recovery
- Performance monitoring and optimization
- S3 lifecycle policies for file cleanup

---

## **Progress Tracking**

- **Phase 1**: ‚è≥ Not Started
- **Phase 2**: ‚è≥ Not Started  
- **Phase 3**: ‚è≥ Not Started
- **Phase 4**: ‚è≥ Not Started
- **Phase 5**: ‚è≥ Not Started
- **Phase 6**: ‚è≥ Not Started

**Overall Progress**: 0% Complete

---

## **Next Steps**

1. ‚úÖ **Review and approve** this implementation plan
2. ‚úÖ **Answer clarifying questions** to finalize design decisions
3. **Begin Phase 1** with database schema implementation
4. **Set up development environment** for pipeline work
5. **Create initial project structure** following existing patterns

## **Ready to Start Implementation** üöÄ

All design decisions have been finalized. The implementation plan is complete and ready for execution. Key decisions include:

- **Google Places API** for geocoding
- **500 records/5MB** file limits with chunked processing
- **30-day staging retention** with archive/delete
- **70% quality threshold** for promotion
- **JSONB configuration** for flexible DMO mappings
- **SQS job queue** for production, manual execution for development
- **Existing patterns** for all integration points

**Recommended starting point**: Begin with Phase 1 - Database Schema Implementation
