Gaps and issues
1. Missing stage: Review ✅ DEFERRED
Design: Manual verification before promotion
Current: No review service or UI
Impact: No manual approval before promotion
Note: This is intentionally deferred as it requires frontend interface for conflict resolution and promotion tasks
2. Transform stage gaps ✅ PARTIALLY COMPLETE
- [x] ✅ Missing deterministic internal ID generation
- [ ] Missing geocoding integration
- [x] ✅ Missing source_id lookup in production DB
- [x] ✅ Missing multi-table field mapping
3. File handling
Missing S3 integration
Missing file naming convention
Missing raw/transformed/validated file management
Missing file versioning and audit trail
4. Import stage gaps
Missing import source management tables
Missing import job tracking
Missing import result tracking
Missing file upload handling
5. Staging stage gaps
Missing deduplication logic
Missing conflict detection
Missing batch processing
Missing ok_to_promote flag management
6. Promotion stage gaps
Missing production dedupe key comparison
Missing merge policy implementation
Missing manually_modified flag respect
Missing bridge table updates
Required additions
1. Missing database tables
2. Missing services
PipelineReviewService — Manual review and approval ✅ DEFERRED (frontend)
PipelineFileService — S3 file management
PipelineGeocodingService — Geocoding integration
PipelineDeduplicationService — Deduplication logic
3. Missing functionality
S3 file upload/download
Geocoding API integration
Production database lookup
Conflict resolution logic
Manual review UI ✅ DEFERRED (frontend)
File naming conventions
Batch processing
Error recovery
Implementation priorities
Phase 1: Core functionality
Add missing database tables
Implement S3 file handling
Add geocoding integration
Implement deduplication logic
Phase 2: Review and promotion ✅ DEFERRED (frontend)
Create review service
Implement conflict resolution
Add manual approval workflow
Complete promotion logic
Phase 3: Advanced features
Batch processing
Error recovery
Performance optimization
Monitoring and alerting
Architecture compliance
Three-layer architecture: implemented
Functional programming: implemented
Service response pattern: implemented
Error handling: implemented
Type safety: implemented
Database queries: implemented
## ✅ TRANSFORM STAGE IMPLEMENTATION COMPLETE

### Implemented Features:
1. **Source ID Generation:** SHA-256 hash of `source_url + source_identifier`
2. **Import Metadata Addition:** Automatic addition of `dmo_id`, `source_id`, `source_location`, `recorded_at`
3. **Bridge Table Lookups:** Lookup existing records in `bridge_source_listing` and `bridge_source_listing_info`
4. **Field Transformation:** Field mapping, normalization, lat/lng rounding
5. **Deterministic IDs:** Generation of internal hash IDs for listings and listing_info
6. **Dedupe Hash Generation:** SHA-256 hashes for content detection
7. **API Endpoints:** 
   - `POST /transform/file` - Transform raw file records
   - `POST /transform/batch` - Transform batch of records
   - `GET /transform/source-id` - Generate source_id from parameters
   - `GET /transform/lookup/:dmoId/:sourceId` - Lookup existing records

### Database Schema Updates:
1. **Bridge Tables:** Updated with `dmo_id`, `source_id`, `source_location` fields
2. **Staging Tables:** Added `internal_id`, `internal_info_id`, `source_id`, `source_location`, `recorded_at`, `dedupe_hash` fields
3. **Indexes:** Added performance indexes for lookup operations

### Updated Summary
The foundation is solid, but several components are missing to meet the design. **Transform stage is now complete** with source ID lookups, field mapping, and deterministic ID generation. The remaining critical gaps are S3 file handling, geocoding integration, and deduplication logic. The Review stage is intentionally deferred as it requires frontend interface for conflict resolution and promotion tasks. Address the backend gaps to reach the design's requirements.




# Clarifying Questions answered during implementation:

## Transform stage
Staging table design for listings and listing_info should include:
(make schema updates in 000036-pipeline-schema.xml)
	•	internal_id (nullable) → references public.listings.id if a match was found.
	•	If no match exists yet, leave internal_id null.
	•	internal_info_id (nullable) → references public.listing_info.id for matched info records.
	•	dedupe_hash → to detect content changes.
	•	ok_to_promote / conflict_flags → for review.
	•	source_id / source_location → to link back to the original import.
Workflow:
	1.	Transform finds a match → fills internal_id/internal_info_id in transform file, used in staging.
	2.	Transform doesn’t find a match → leave internal_id null; staging row is new.
	3.	During Promotion:
	•	Rows with existing internal_id → update existing listing.
	•	Rows with null internal_id → insert new listing, then update internal_id in staging for bridge table tracking.

This allows staging to track both matched and new records in a single table.

Clarifying questions:

Source location structure: stored as separate columns (source_url, source_identifier)

source_id generation: use SHA-256 
Hash the core identifying fields only: source_url + source_identifier

DMO_id in bridge tables:  Include dmo_id in both bridge_source_listing and bridge_source_listing_info. Index (dmo_id, source_id) for fast lookup.

deterministic internal ID generation:
	1.	For public.listings.id
	•	Keep the existing serial table ID as the primary key.
	•	Generate a dedupe key/hash separately (e.g., SHA-256) from normalized fields such as:
	•	Name
	•	Address
	•	Lat/Lng (rounded for tolerance)
	•	Include dmo_id only if you need DMO-scoped deduplication; otherwise, hash should represent the listing itself globally.
	2.	For public.listing_info.id
	•	Keep table ID as primary key.
	•	Generate a hash/dedupe key based on content fields (description, opening hours, phone, etc.).
	•	Link multiple listing_info rows to the same listing via listing_id FK.
	3.	Collision handling:
	•	Treat collisions as updates to existing row if dedupe key matches.
	•	If hash collision occurs between two truly different entities (rare for SHA-256), fallback to:	Append a small unique suffix (e.g., sequence)

Transform Stage Flow :
	1.	Import Stage
	•	Load raw data from CSV/API.
	  •	Add DMO context and source metadata:
	  •	dmo_id → identifies which DMO the data belongs to.
	  •	source_url → e.g., WordPress URL or API endpoint.
	  •	source_identifier → e.g., external system ID (like Priceline ID or wordpress slug).
	  •	source_id → SHA-256 hash of source_url + source_identifier.
	 •	recorded_at → timestamp of import.
	2.	Transform Stage
	•	Field mapping & normalization: map source fields → internal schema, normalize strings, round lat/lng, etc.
	•	Lookup existing entities:
	•	Check if a record already exists in public.listings via source_id.
	•	If a match exists:
	•	Populate location_id and contact_info_id with existing IDs.
	•	If no match exists:
	•	Assign temporary location_id and contact_info_id for new records (used for staging and later promotion).
	•	Generate deterministic internal IDs for listings and listing_info based on normalized fields, hashes, and optionally dmo_id.
	•	Optional geolocation lookup:
	•	Only performed for new records missing lat/lng/address.
	•	Add validation flags if using a separate validation step (data types, required fields, business rules, quality scoring).
	3.	Staging
	•	Load transformed data with all resolved/internal references:
	•	location_id
	•	contact_info_id
	•	ok_to_promote flag (default based on validation/quality score)
	•	Keep full audit fields for review and deduplication.
	4.	Promotion
	•	Insert new records into public.listings and public.listing_info for rows with temporary IDs.
	•	Update staging rows with real serial IDs after insertion.
	•	Update bridge tables (bridge_source_listing, bridge_source_listing_info) linking source_id → internal IDs.
	•	Respect merge policies, manual edits, and conflict resolutions.

Bridge table population
The bridge tables should be populated during promotion, not during transform:
	•	Reasoning:
	•	During transform, you may only have temporary or unresolved IDs (location_id, contact_info_id) for new records. These aren’t final, so linking them in bridge tables would be inaccurate.
	•	Once the promotion stage inserts rows into the production tables (public.listings, public.listing_info) and the real serial IDs are assigned, you can correctly populate the bridge tables (bridge_source_listing, bridge_source_listing_info) linking source_id → internal IDs.

Existing record handling:
	If source_id exists in bridge tables:
	•	Update the existing listing/listing_info only if the hash of relevant fields indicates a change (i.e., the record has updated data).
	•	Do not blindly overwrite—respect manually edited data.

## Fixes to those changes:

lets clarify some behavior, which will affect schemas and tables. make adjustments to schemas and code that is affected:

1. We decided to replace source_location with 2 fields: source_url and source_identifier.
- Import file: should contain source_url and source_identifier.
- Transform file: use these two fields to compute source_id via SHA-256 hash.
- Staging tables: store source_id, source_url, source_identifier, internal_id (from matching algorithm, or null for no match)

Transform file = raw file + transformations + added metadata:
	•	dmo_id
	•	source_url
	•	source_identifier
	•	source_id (SHA-256 hash of the two above)
	•	internal_id / internal_info_id (from matching lookups, or empty if new)
	•	dedupe_hash
	•	normalized/mapped fields, lat/lng, etc.
Load Staging Phase:
	•	Reads only the transform file.
	•	All the necessary info (including source fields) is already there.


2. we need an additional matching method to determine if a match exists if a bridge table lookup fails:

**Revised Transform Matching Flow**
	1.	Map fields from raw import.
	2.	Normalize key fields and generate dedupe hash.
	3.	Lookup by source_id in bridge_source_listing / bridge_source_listing_info.
	•	If found → populate internal_id.
	4.	If no bridge match, lookup by dedupe hash in public.listings / public.listing_info.
	•	If a match exists → populate internal_id from the existing record.
	•	Mark this as a potential new bridge entry (source_id → internal_id).
	5.	If no match anywhere → leave internal_id empty → will create new record on promotion.

3. separate dedup and change_detection hashes are needed:
	1.	Transform stage:
	•	Compute dedupe hash → for matching existing records.
	•	Compute full hash → for detecting updates once matched.
	2.	Lookup in bridge/public tables:
	•	If dedupe match exists → compare full hash with stored hash.
	•	If different → mark ok_to_promote or flag for review.

## ✅ CLARIFIED BEHAVIOR IMPLEMENTATION COMPLETE

### Schema Updates Completed:
1. **Bridge Tables:** Clarified to store only lookup relationships (`dmo_id`, `source_id`, `listing_id`/`listing_info_id`) - **source_url/source_identifier removed as they belong in raw/transform/staging files**
2. **Staging Tables:** Updated to use `source_url`, `source_identifier`, and added `change_hash` fields
3. **Database Indexes:** Added performance indexes for dedupe and change hash lookups

### Transform Service Enhancements:
1. **Separate Source Fields:** Import metadata now includes separate `source_url` and `source_identifier`
2. **Dedupe Hash Fallback:** Added fallback matching using dedupe hash lookup in public tables
3. **Dual Hash System:** Separate `dedupe_hash` (for matching) and `change_hash` (for content comparison)
4. **Revised Matching Flow:**
   - Try bridge table lookup by `source_id` first
   - If no match, try dedupe hash lookup in public tables
   - If no match anywhere, leave `internal_id` null for new record creation

### New DAO Components:
1. **DedupeDao:** New DAO for dedupe hash lookups in public tables
2. **Enhanced Queries:** Updated queries for new schema fields
3. **Type Safety:** Updated TypeScript interfaces to reflect new field structure

### Key Technical Improvements:
- **Source ID Generation:** SHA-256 hash of `source_url + source_identifier`
- **Deterministic Matching:** Dedupe hash based on normalized content fields
- **Change Detection:** Change hash for detecting content updates
- **Fallback Matching:** Bridge table lookup → Public dedupe hash lookup → New record creation
- **Multiple Hash Types:** `dedupe_hash` for matching, `change_hash` for change detection


# More clarifying updates:
lets do some organization and clarifying:

replace the originally designed staging tables with this new suggested staging schema, and update services to use this new staging schema design. make sure to include rollback sql.  make changes in the 

we will also need new public schema fields - add new hash fields to the public objects that correspond to the staging hash fields. ✅ COMPLETED

✅ SCHEMA IMPLEMENTATION COMPLETE:
- Recreated 000036-pipeline-schema.xml with proper rollback SQL and RBAC permissions
- Created 000037-staging-schema.xml with new listings_flat table design  
- Created 000039-public-schema-hash-fields.xml to add hash fields to public tables
- Updated DAO queries for new hash field names
- Updated transform service with new match hash flow
- Fixed all linting errors in pipeline routes

**suggested staging schema**
```sql
CREATE TABLE staging.listings_flat (
    -- Primary key for staging row
    id serial NOT NULL PRIMARY KEY,

    -- Source information
    dmo_id integer NOT NULL,
    source_id varchar(64) NOT NULL, -- SHA-256 hash of source_url + source_identifier
    source_url text NULL,
    source_identifier text NULL,
    -- Timestamps for auditing
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()

    -- Internal IDs linking to public tables (may be NULL if new record)
    listing_internal_id integer NULL,
    listing_info_internal_id integer NULL,
    location_internal_id integer NULL,
    contact_info_internal_id integer NULL,

    -- Hashes for matching (and deduplication)
    listing_match_hash varchar(64) NULL,
    listing_info_match_hash varchar(64) NULL,
    
    -- Hashes for change detection
    listing_content_hash varchar(64) NULL,
    location_content_hash varchar(64) NULL,
    listing_info_content_hash varchar(64) NULL,
    contact_info_content_hash varchar(64) NULL,
    images_content_hash varchar(64) NULL,
    opening_dates_content_hash varchar(64) NULL,
    opening_hours_content_hash varchar(64) NULL,
    attributes_content_hash varchar(64) NULL,
    
    -- Per-entity promotion/action status
    promote_status varchar(16) DEFAULT 'pending', -- overall row status: pending/insert/update/delete
    listing_action varchar(16) DEFAULT 'pending', 
    listing_info_action varchar(16) DEFAULT 'pending',
    location_action varchar(16) DEFAULT 'pending',
    contact_info_action varchar(16) DEFAULT 'pending',
    images_action varchar(16) DEFAULT 'pending',
    opening_dates_action varchar(16) DEFAULT 'pending',
    opening_hours_action varchar(16) DEFAULT 'pending',
    attributes_action varchar(16) DEFAULT 'pending',
    
    -- Validation & quality control
    validation_errors text NULL, -- JSON or text describing validation failures
    quality_score numeric(5,2) NULL,
    manual_review_required boolean DEFAULT false,

    -- Raw content fields (optional, depending on pipeline needs)
    listing_name varchar(255) NULL,
    listing_info_description text NULL,
    loc_address1 varchar(255) NULL,
    loc_address2 varchar(255) NULL,
    loc_region varchar(100) NULL,
    loc_city varchar(100) NULL,
    loc_state varchar(50) NULL,
    loc_zip varchar(20) NULL,
    loc_country char(2) NULL,
    loc_lat numeric(10,8) NULL,
    loc_lng numeric(11,8) NULL,
    ci_name character varying NULL,
    ci_email character varying NULL,
    ci_local_phone character varying NULL,
    ci_international_phone character varying NULL,
    ci_created_at timestamp without time zone NULL,
    ci_updated_at timestamp without time zone NULL,
    ci_website_url character varying NULL,
    ci_facebook_url character varying NULL,
    ci_instagram_url character varying NULL,
    ci_youtube_url character varying NULL,
    ci_tiktok_url character varying NULL,
    ci_twitter_url character varying NULL,
    ci_pinterest_url character varying NULL,
    ci_tripadvisor_url character varying NULL,
    ci_yelp_url character varying NULL,
    images_images jsonb, -- [id, listing_info_id, image, rank]
    dates jsonb,
    hours jsonb,
    attributes jsonb
);

-- 1. Ensure unique staging row per source
CREATE UNIQUE INDEX idx_staging_source_dmo ON staging.listings_flat(dmo_id, source_id);

-- 2. Lookups for linking to production internal IDs
CREATE INDEX idx_staging_listing_internal ON staging.listings_flat(listing_internal_id);
CREATE INDEX idx_staging_listing_info_internal ON staging.listings_flat(listing_info_internal_id);
CREATE INDEX idx_staging_location_internal ON staging.listings_flat(location_internal_id);
CREATE INDEX idx_staging_contact_info_internal ON staging.listings_flat(contact_info_internal_id);

-- 3. Deduplication / match hash lookups
CREATE INDEX idx_staging_listing_match_hash ON staging.listings_flat(listing_match_hash);
CREATE INDEX idx_staging_listing_info_match_hash ON staging.listings_flat(listing_info_match_hash);

-- 4. Promotion / review queries
CREATE INDEX idx_staging_promote_status ON staging.listings_flat(promote_status);
CREATE INDEX idx_staging_manual_review ON staging.listings_flat(manual_review_required);

-- 5. JSONB content indexes (if querying inside arrays)
CREATE INDEX idx_staging_images_jsonb ON staging.listings_flat USING gin(images_images);
CREATE INDEX idx_staging_dates_jsonb ON staging.listings_flat USING gin(dates);
CREATE INDEX idx_staging_hours_jsonb ON staging.listings_flat USING gin(hours);
CREATE INDEX idx_staging_attributes_jsonb ON staging.listings_flat USING gin(attributes);
```

let's reclarify - we need 2 types of hash fields, in both public and staging tables: id_hash for dedup and matching logic, and content_hash to figure out if content changed.

we need id_hash on tables: listings (name + location.address1 + location.city), listing_info (dmo_id + listing_id)

we need content_hash on tables: listings(name), listing_info(description), locations(address1 + address2 + city + state + zip + country + lat + lng), contact_info (name + email + local_phone + international_phone + website_url + facebook_url + instagram_url + youtube_url + tiktok_url + twitter_url + pinterest_url + tripadvisor_url + yelp_url )

transform stage:
- if a listing match is not found with the bridge table lookup, then we need to also try to find a match with the match_hash.
- once a listing is found, try to find a match for the listing_info.
- if a listing match is found then a content match can be performed on the location
- if a listing_info match is found then a content match can be performed on the contact_info.

## ✅ FINAL MATCHING LOGIC IMPLEMENTED

**Direct listing_info lookup approach:**
- ✅ Only look for listing_info match once a listing match is found
- ✅ Match is found where `listing_info.listing_id = listing internal id AND listing_info.dmo_id = source dmo_id`
- ✅ Implemented `getPublicListingInfoByListingDmo()` method for direct lookup
- ✅ Eliminates need for `bridge_source_listing_info` table complexity
- ✅ Once listing match is found → location object is known
- ✅ Once listing_info match is found → contact_info object is known

**Updated transform service logic:**
1. Bridge table lookup by `source_id` (if exists)
2. If no bridge match: Try `listing_match_hash` lookup in public tables
3. If listing found: Direct `listing_info` lookup using `listing_id + dmo_id`
4. If listing found: Content hash comparison for `location`
5. If listing_info found: Content hash comparison for `contact_info`

**Key improvements:**
- Simplified listing_info matching eliminates bridge table dependency
- Direct database lookup using `getPublicListingInfoByListingDmo(listingId, dmoId)`
- Cleaner separation between bridge table (`source_id` ↔ `listing_id`) and internal matching (`listing_id + dmo_id` → `listing_info_id`)

## ✅ REMOVED REDUNDANT COMPONENTS

**Confirmed elimination of:**
- ✅ `bridge_source_listing_info` table (000040-remove-listing-info-redundancy.xml)
- ✅ `listing_info_match_hash` field from staging schema  
- ✅ `listing_info_match_hash` field from public listing_info table
- ✅ `generateListingInfoMatchHash()` function from transform service
- ✅ Related indexes and unused code

**Simplified matching flow:**
1. Bridge table lookup by `source_id` (for listings only)
2. Direct `listing_match_hash` lookup in public listings
3. **Direct `listing_id + dmo_id` lookup for listing_info** (no hash needed)
4. Content hash comparison for location, listing_info, contact_info

**Schema cleanup:**
- Pipeline schema: Removed `bridge_source_listing_info` table
- Staging schema: Removed `listing_info_match_hash` field
- Public schema: Removed `listing_info_match_hash` field  
- Transform service: Removed unused hash generation code

This creates a much cleaner, more maintainable pipeline with straightforward logic!

## ✅ SCHEMA CLEANUP COMPLETE

**Updated all schema files to remove unnecessary double quotes:**
- ✅ Pipeline schema: Clean `pipeline.table_name` syntax
- ✅ Staging schema: Clean `staging.table_name` syntax  
- ✅ Public schema: Clean `public.table_name` syntax
- ✅ All column names: Clean lowercase without quotes
- ✅ All foreign key references: Clean lowercase without quotes
- ✅ All constraint names: Clean without quotes
- ✅ All index names: Clean without quotes

**Consistent schema naming convention:**
```sql
-- Schema creation
CREATE SCHEMA IF NOT EXISTS pipeline;

-- Table creation  
CREATE TABLE pipeline.job_queue (
  job_id SERIAL PRIMARY KEY,
  dmo_id INT NOT NULL REFERENCES public.dmos(id),
  ...
);

-- Index creation
CREATE INDEX idx_job_queue_pending ON pipeline.job_queue(status, next_attempt_at);
```

Final result: Clean, readable Liquibase schemas following PostgreSQL best practices!

