# Listings Pipeline Design
**goal:** implement an enterprise grade listings pipeline with a powerful graphical interface for managing data pipeline.

**Data pipeline**
- **configure** import parameters for a dmo
- **import** to raw file
- **transform** to transformed file
- **stage** to staging schema
- **accept** staging data and specify conflict actions
- **promote** to public schema
- **review** public data origins and updated dates etc.


## Minimum Viable Pipeline
implement the simplest possible working version of the pipeline that we can enhance later.

This means that we will only do a simple csv upload, only update limited fields, only stage listings (not listing_info).

So add code for a basic pipeline, but everything should be hooked up to real data and real methods and apis, implement proper layered architecture, follow all codebase patterns and standards. Each step needs to be functional, connected to real methods and api calls, and not use mock or test data just for display.  

Additional points for Cursor to implement directly
	•	Implement real geocoding API calls in Transform stage if lat/lng missing
	•	Ensure staging.status field is respected during manual review
	•	Idempotency: repeated promotions should not create duplicates
	•	Job queue workers: handle retry_count, locked_by, dequeue_attempts, next_attempt_at
	•	Admin UI: simple staging review table with promote button
	•	Only a minimal subset of listing fields needs to be handled for MVP (name, address, lat, lng, category)

  
## Minimal Listings Pipeline

We’ll do five main stages:
	1.	Upload CSV → Raw File
	2.	Transform → Transformed File
	3.	Stage/Load → Staging Schema (manual review optional)
  4.  Review - manual verification of stage data, confirm the updates, additions, deletes, and click the 'promote' button
	5.	Promote → Stage schema data -> Public Schema

⸻

1. Upload CSV → Raw File

Component: Upload service / script

Input: CSV with columns: name, address, lat, lng, category

Actions / Methods:
	•	Read CSV file
	•	Assign source_name (e.g., ‘user_csv_upload’)
	•	Generate deterministic source_id per row (hash of source_name + name + address + lat/lng)
	•	Save raw records to file (JSON or CSV) with fields: source_name, source_id, name, address, lat, lng, category, received_at

Output: Raw file ready for transformation

⸻

2. Transform → Transformed File

Component: Transformation script

Input: Raw file

Actions / Methods:
	•	Normalize fields (name, address, round lat/lng)
	•	Compute content_hash (hash of normalized fields: name + address + lat + lng + category)
	•	Compute dedupe_key for cross-source matching (hash excluding source_name)
	•	Optional geocode lookup if lat/lng missing
	•	Save transformed JSON with fields: source_name, source_id, name, address, lat, lng, category, content_hash, dedupe_key

Output: Transformed JSON ready for staging

⸻

3. Load → Staging Schema (Matching / Deduplication Happens Here)

Component: Staging DB / loader

Input: Transformed JSON file

Actions / Methods:
	•	For each record:
	•	Match against staging table or public table by dedupe_key
	•	Determine action:
	•	Insert: new listing → assign listing_id
	•	Update: existing listing → update fields if content_hash changed
	•	No change: keep as-is
	•	Save staging record with fields: listing_id, source_name, source_id, dedupe_key, content_hash, name, address, lat, lng, category, status
	•	Optional: flag inconsistencies for manual review

Matching / Deduplication Logic:
	•	Deduplicate within staging using dedupe_key
	•	Assign same listing_id to duplicates
	•	Track origin via source_id

⸻

4. Manual Review of Staging Records

Component: Admin UI / review script

Input: Staging records (all or flagged for review)

Actions / Methods:
	•	Review records flagged for conflicts, missing fields, or potential duplicates
	•	Set status and promotion action for each record:
	•	Promote → ready for public schema
	•	Do not promote / Hold → keep in staging for further review
	•	Optional: Merge → combine duplicates manually before promotion
	•	Update staging table with status and notes

Output: Staging records with clear promotion instructions

⸻

5. Promote → Public Schema

Component: Promotion script / service

Input: Staging records marked Promote

Actions / Methods:
	•	For each record:
	•	Match dedupe_key in public table
	•	If exists → update fields if content_hash changed
	•	If not → insert new listing_id
	•	Update bridge table (listing_source_map) with listing_id, source_name, source_id, last_seen
	•	Set timestamps (created_at, updated_at)

Notes:
	•	Deduplication already happened at Load → Staging stage
	•	Promotion is now mostly insert / update, following manual review actions


Matching / Deduplication Order
	1.	Transform → Staging:
	•	Compute dedupe_key
	2.	Load into Staging:
	•	Match dedupe_key to existing staging/public listings
	•	Assign listing_id
	•	Determine insert / update / no-change
	3.	Staging → Public:
	•	Insert new listings, update changed listings
	•	Update bridge tables


### S3 file naming

s3://{environment: dev, staging, app}-trippl-data/{stage: raw, transformed}/{data-domain: listings, reviews, events}/{country: xx}/{state/province: xx}/{dmo-id}/{date: YYYYMMDD}/{batch: nnn}-{timestamp: YYYYMMDDTHHMMSS}.{format: json, csv, xls}.gz

### S3 image naming

s3://{environment: dev, staging, app}-trippl-images/{country}/{state}/{dmo-id}/{listing-id}/{filename}.{ext}



## Admin pages
add permissions for new pipeline functions. add all permissions to the super_admin role.

basic permission needed to view and access pipeline menus and pages is pipeline:read

make a new sidebar menu section called Pipeline, visible to users with permission

make admin pages to manage all steps of the pipeline. admin pages should follow the structure of existing admin pages (topbar, sidebar etc)

## Staging / Public Tables**

Mirrored structure in staging and public:

Key Principles:
- Staging mirrors production schema for safe promotion.
- Staging holds all rows for the batch being ingested, not just deltas.
- bridge_source_listing and hashes track source record → canonical listing mapping.

# Design things that came up after the initial design

## job scheduling
use manual scheduling/execution on localhost.

on dev/staging/app use aws SQS to schedule and execute jobs in the job_queue

**if dmo config changes** then add a dialog at save time that checks for scheduled jobs and lets user ignore or cancel.  Changes to schedule shouldn't be an issue, though an old job with old config might overwrite new job(s).



## pipeline schema table design

```sql

example config_params:
"import": {
  "source_type": "csv",   -- 'csv', 'wordpress', 'drupal', 'website', 'api', 'eventbrite'
  "file_format": "csv",   -- 'csv', 'json'
  "delimiter": ",",
  "encoding": "utf-8",
  "skip_header": true
}

"transform": {
  "normalize_fields": ["name", "address", "category"],
  "round_lat_lng": 5,
  "hash_fields": ["name", "address", "lat", "lng", "category"],
  "dedupe_key_fields": ["name", "address", "lat", "lng"]
}

"staging": {
  "match_on": ["dedupe_key"],
  "conflict_resolution": "update_if_changed",
  "status_field": "ok_to_promote"
}

"promotion": {
  "merge_policy": "latest_wins",
  "update_bridge_table": true
}

CREATE TABLE pipeline.dmo_configs (
id              SERIAL PRIMARY KEY,                       – unique config row
dmo_id          INT NOT NULL REFERENCES public.dmos(id),  – which DMO this config belongs to
type            VARCHAR(20) NOT NULL,                     – stage type: ‘import’, ‘transform’, ‘stage’, ‘promote’
data_domain     TEXT NOT NULL,                            – e.g., ‘listings’, ‘events’, ‘reviews’
config_params   JSONB NOT NULL,                           – JSON with rules relevant to this stage
version         INT NOT NULL DEFAULT 1,                   – increment if changing this config
schedule        TEXT NOT NULL,                             – e.g., ‘manual’, ‘pipeline’, ‘hourly’, ‘daily’, ‘weekly’, ‘monthly’
active          BOOLEAN DEFAULT TRUE,                     – is this config currently active?
created_at      TIMESTAMPTZ DEFAULT now(),                – timestamp when created
updated_at      TIMESTAMPTZ DEFAULT now()                 – timestamp when last updated
);

ALTER TABLE pipeline.dmo_configs
ADD CONSTRAINT type_enum CHECK (type IN (‘import’, ‘transform’, ‘stage’, ‘promote’));
– ensures type is valid

⸻

CREATE TABLE pipeline.job_queue (
job_id          SERIAL PRIMARY KEY,                       – unique job identifier
job_type        VARCHAR(50) NOT NULL,                     – ‘import_raw’, ‘transform’, ‘load_stage’, ‘promote_public’
status          VARCHAR(20) NOT NULL DEFAULT ‘pending’,   – job status: pending, running, completed, failed, cancelled
priority        INT DEFAULT 0,                             – optional job priority
source_name     TEXT,                                      – optional name of data source
enqueued_at     TIMESTAMPTZ DEFAULT now(),                 – timestamp when job was added to queue
started_at      TIMESTAMPTZ,                               – timestamp when job started
finished_at     TIMESTAMPTZ,                               – timestamp when job finished
dequeue_attempts INT DEFAULT 0,                             – number of times job was dequeued
next_attempt_at TIMESTAMPTZ,                               – when to try next if failed
locked_by       TEXT,                                      – worker that locked the job
locked_at       TIMESTAMPTZ,                               – timestamp when job was locked
retry_count     INT DEFAULT 0,                              – total retries attempted
error_message   TEXT,                                      – summary error message if job failed
config_snapshot_id UUID REFERENCES pipeline.job_configs(snapshot_id) ON DELETE SET NULL
);

CREATE INDEX idx_job_queue_pending ON pipeline.job_queue(status, next_attempt_at);
– helps workers quickly find pending jobs that are ready to run

⸻

CREATE TABLE pipeline.job_configs (
snapshot_id     UUID PRIMARY KEY DEFAULT gen_random_uuid(), – unique snapshot identifier
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
dmo_id          INT NOT NULL REFERENCES public.dmos(id),   – which DMO this snapshot belongs to
type            VARCHAR(20) NOT NULL,                      – stage type: ‘import’, ‘transform’, ‘stage’, ‘promote’
data_domain     TEXT NOT NULL,                              – e.g., ‘listings’, ‘events’, ‘reviews’
config_params   JSONB NOT NULL,                             – copied config for reproducibility
version         INT NOT NULL DEFAULT 1,                     – snapshot version
schedule        TEXT NOT NULL,                               – schedule info from original config
created_at      TIMESTAMPTZ DEFAULT now(),                  – when snapshot was created
updated_at      TIMESTAMPTZ DEFAULT now()                   – last updated timestamp
);

⸻

CREATE TABLE pipeline.files (
id              SERIAL PRIMARY KEY,                         – unique file record
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
file_path       TEXT NOT NULL,                               – path to file
stage           VARCHAR(20) NOT NULL,                        – stage: ‘import’, ‘transform’, ‘stage’, ‘promote’
data_domain     TEXT,                                        – optional: ‘listings’, ‘events’, ‘reviews’
country         TEXT,                                        – optional country info
dmo_id          INT,                                         – optional DMO ID
file_date       DATE,                                        – optional file date
batch           INT,                                         – optional batch number
format          VARCHAR(10),                                 – file format: ‘csv’, ‘json’, ‘xls’
file_size       BIGINT,                                      – size in bytes
record_count    INT,                                         – number of records in file
file_hash       TEXT,                                        – optional hash for deduplication
created_at      TIMESTAMPTZ DEFAULT now()                    – timestamp file was created
);

CREATE INDEX idx_files_job_stage ON pipeline.files(job_id, stage);
– fast lookup for files per job and stage

⸻

CREATE TABLE pipeline.job_results (
job_id          INT PRIMARY KEY REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
records_total   INT,                                         – total records processed
records_new     INT,                                         – new records inserted
records_updated INT,                                         – updated records
records_deleted INT,                                         – deleted records
records_duplicates INT,                                      – duplicates detected
records_unchanged INT,                                       – unchanged records
records_error   INT,                                         – records with errors
created_at      TIMESTAMPTZ DEFAULT now()
);

⸻

CREATE TABLE pipeline.job_errors (
id              SERIAL PRIMARY KEY,
job_id          INT NOT NULL REFERENCES pipeline.job_queue(job_id) ON DELETE CASCADE,
file_id         INT REFERENCES pipeline.files(id),           – file where error occurred
record_id       TEXT,                                        – unique record identifier / hash
original_record JSONB,                                       – full original record
error_type      TEXT,                                        – type of error, e.g., parse_error
error_message   TEXT,                                        – descriptive error message
created_at      TIMESTAMPTZ DEFAULT now()
);

⸻

CREATE TABLE pipeline.bridge_source_listing (
id                  SERIAL PRIMARY KEY,
job_file_id         INT NOT NULL REFERENCES pipeline.files(id),
source_record_hash  TEXT NOT NULL,                           – hash of source record for mapping
listing_id          INT REFERENCES public.listings(id),       – canonical listing ID
certainty_score     NUMERIC(5,4) DEFAULT 1.0,                – confidence 0.0–1.0
mapped_at           TIMESTAMPTZ DEFAULT now(),               – when mapping occurred
error_type          TEXT,                                     – optional mapping error type
error_message       TEXT,                                     – optional error message
UNIQUE (job_file_id, source_record_hash)
);

CREATE INDEX idx_bridge_listing ON pipeline.bridge_source_listing(listing_id);
– fast lookup for canonical listings

```

# how our staging -> public promotion works
1. Import Stage
	•	Input: Raw CSV/JSON/API/scrape files.
	•	Actions:
	•	Save raw files with metadata: file_id, source_name, dmo_id, file_date, format, record_count.
	•	Enqueue import_raw job with config snapshot.
	•	Output: Raw files stored in file system or DB.

⸻

2. Transform Stage
	•	Input: Raw files.
	•	Actions:
	•	Normalize fields (name, address, category, etc.).
	•	Compute hashes (hash_fields) and dedupe keys (dedupe_key_fields).
	•	Convert to standard JSON format for staging.
	•	Enqueue transform job.
	•	Output: Transformed JSON files ready for staging.

⸻

3. Stage Stage
	•	Input: Transformed files.
	•	Actions:
	•	Load into staging.listings and staging.listing_info.
	•	Compute deterministic matches with public.listings and public.listing_info.
	•	Assign certainty_score for each match.
	•	Populate bridge_source_listing table mapping staging → public IDs.
	•	Link dependent objects (contact_info, images, hours) to staging records (no matching).
	•	Mark ok_to_promote = true or flag conflicts for manual review.
	•	Enqueue load_stage job.
	•	Output: Staging tables populated with matches, deduped, ready for promotion.

⸻

4. Promote Stage
	•	Input: staging records with ok_to_promote = true.
	•	Actions:
	•	Update/insert public.listings and public.listing_info using bridge table.
	•	Insert/update dependent objects (contact_info, images, hours) following mapped public IDs.
	•	Enqueue promote_public job.
	•	Ensure idempotency: repeated runs do not duplicate data.
	•	Output: Canonical records in public tables updated.

⸻

Cursor Implementation Details
	•	Keys & Hashes: staging_id, source_id, hash, dedupe_key for listings and listing_info.
	•	Bridge Table: staging.listings.id → public.listings.id and staging.listing_info.id → public.listing_info.id with certainty_score.
	•	Job Queue: Track job_type, status, file_id, stage, dmo_id, config_snapshot_id, timestamps.
	•	Error Handling: job_errors table for failed or conflicting records.
	•	Timestamps: created_at, updated_at in all staging/public tables for audit and idempotency.
	•	Deterministic Matching: Only listings and listing_info — all dependent objects follow matched public IDs.